/* gstreamer-imx: GStreamer plugins for the i.MX SoCs
 * Copyright (C) 2019  Carlos Rafael Giani
 *
 * This library is free software; you can redistribute it and/or
 * modify it under the terms of the GNU Library General Public
 * License as published by the Free Software Foundation; either
 * version 2 of the License, or (at your option) any later version.
 *
 * This library is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * Library General Public License for more details.
 *
 * You should have received a copy of the GNU Library General Public
 * License along with this library; if not, write to the Free
 * Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 */

#include <gst/gst.h>
#include <gst/allocators/allocators.h>
#include <gst/video/gstvideodecoder.h>
#include <gst/video/gstvideometa.h>
#include <imxdmabuffer/imxdmabuffer.h>
#include <imxdmabuffer/imxdmabuffer_config.h>
#include <imxvpuapi2/imxvpuapi2.h>
#include "gst/imx/common/gstimxdmabufferallocator.h"
#include "gstimxvpudec.h"
#include "gstimxvpudeccontext.h"
#include "gstimxvpudecbufferpool.h"
#include "gstimxvpucommon.h"


GST_DEBUG_CATEGORY_STATIC(imx_vpu_dec_debug);
#define GST_CAT_DEFAULT imx_vpu_dec_debug


typedef enum
{
	/* Thread has not been started. */
	GST_IMX_VPU_DEC_DECODING_THREAD_STATE_INACTIVE = 0,
	/* Thread is running normally. Pushing frames to the encoded frame queue is possible. */
	GST_IMX_VPU_DEC_DECODING_THREAD_STATE_RUNNING,
	/* Decoder is being drained. Pushing frames to the encoded frame queue is not possible.
	 * This changes back to the RUNNING state once draining completes. */
	GST_IMX_VPU_DEC_DECODING_THREAD_STATE_DRAINING,
	/* Thread is being stopped. Pushing frames to the encoded frame queue is not possible.
	 * Any queued frames are discarded and do not get decoded. */
	GST_IMX_VPU_DEC_DECODING_THREAD_STATE_STOPPING,
	/* Decoding failed because of an error. The thread is stopped. This state is used
	 * for informing the main streaming thread that there was a failure. */
	GST_IMX_VPU_DEC_DECODING_THREAD_STATE_FAILED
}
GstImxVpuDecDecodingThreadState;


#define DECODING_THREAD_LOCK(imx_vpu_dec) g_mutex_lock(&((imx_vpu_dec)->decoding_thread_mutex))
#define DECODING_THREAD_UNLOCK(imx_vpu_dec) g_mutex_unlock(&((imx_vpu_dec)->decoding_thread_mutex))
#define DECODING_THREAD_SIGNAL(imx_vpu_dec) g_cond_broadcast(&((imx_vpu_dec)->decoding_thread_cond))
#define DECODING_THREAD_WAIT(imx_vpu_dec) g_cond_wait(&((imx_vpu_dec)->decoding_thread_cond), &((imx_vpu_dec)->decoding_thread_mutex))


/* This is the base class for decoder elements. Derived classes
 * are not implemented manually. Rather, they are procedurally
 * generated out of information from the GstImxVpuCodecDetails
 * table that is stored in gstimxvpucommon.c and access by calling
 * gst_imx_vpu_get_codec_details(). The autogeneration and
 * registration is done by gst_imx_vpu_dec_register_decoder_type().
 * To let the autogenerated subclass know what format it is supposed
 * to handle, the libimxvpuapi compression format enum is stored
 * as qdata in the class derived from GstImxVpuDecClass. That qdata
 * is accessed using gst_imx_vpu_compression_format_quark().
 *
 * The actual decoding is performed in a separate thread to allow
 * downstream to process a frame (which may block the srcpad) while
 * the next frame is being decoded at the same time, thus lowering
 * the latency of the whole decoding. */


struct _GstImxVpuDec
{
	GstVideoDecoder parent;

	/* Out-of-band codec data along with mapping information.
	 * See the code in gst_imx_vpu_dec_set_format() for details. */
	GstBuffer *codec_data;
	GstMapInfo codecdata_map_info;
	gboolean codec_data_is_mapped;

	/* Input and output video codec states. The input state is
	 * set in gst_imx_vpu_dec_set_format(). The output state is
	 * set in gst_imx_vpu_dec_decode_queued_frames() once the
	 * IMX_VPU_API_DEC_OUTPUT_CODE_NEW_STREAM_INFO_AVAILABLE
	 * output code is received. */
	GstVideoCodecState *input_state;
	GstVideoCodecState *output_state;

	/* Copy of the stream info received when the output code
	 * IMX_VPU_API_DEC_OUTPUT_CODE_NEW_STREAM_INFO_AVAILABLE
	 * appears. */
	ImxVpuApiDecStreamInfo current_stream_info;

	/* Current decoder context. Created in
	 * gst_imx_vpu_dec_set_format(). */
	GstImxVpuDecContext *decoder_context;

	/* Current DMA buffer pool. Created in
	 * gst_imx_vpu_dec_decide_allocation(). */
	GstImxVpuDecBufferPool *dma_buffer_pool;
	/* The "nonvideometa pool" is used when the frames the VPU
	 * outputs aren't "tightly packed". See need_to_copy_output_frames
	 * below for details. */
	GstBufferPool *nonvideometa_output_buffer_pool;
	GstVideoInfo nonvideometa_output_video_info;

	/* The output buffer that is prepared for receiving
	 * a decoded output frame. This is only used if the
	 * IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_DECODED_FRAMES_ARE_FROM_BUFFER_POOL
	 * global imxvpuapi decoder flag is _not_ set. See the
	 * libimxvpuapi documentation for more details. */
	GstBuffer *prepared_output_buffer;

	/* The stream buffer that is needed by the decoder for all
	 * of its decoding operations. Created in gst_imx_vpu_dec_start(). */
	GstMemory *stream_buffer;
	/* The actual libimxvpuapi decoder. Created in
	 * gst_imx_vpu_dec_set_format(). Gets destroyed in
	 * the decoder context' finalizer. */
	ImxVpuApiDecoder *decoder;
	/* Pointer to the constant, static global decoder
	 * information from libimxvpuapi. */
	ImxVpuApiDecGlobalInfo const *dec_global_info;
	/* The parameters that are passed on to the imx_vpu_api_dec_open()
	 * call that opens a libimxvpuapi decoder instance. */
	ImxVpuApiDecOpenParams open_params;
	/* libimxdmabuffer-based DMA buffer allocator that is used for
	 * allocating the stream buffer and the VPU framebuffer pool buffers.
	 * Depending on the configuration, this may or may not be the
	 * DMA-BUF backed GstImxIonAllocator. */
	GstAllocator *default_dma_buf_allocator;

	/* Sometimes, even after one of the GstVideoDecoder vfunctions
	 * reports an error, processing continues. This flag is intended
	 * to handle such cases. If set to TRUE, several functions such as
	 * gst_imx_vpu_dec_handle_frame() will exit early. The flag is
	 * cleared once the decoder is restarted. */
	gboolean fatal_error_cannot_decode;

	/* true if the VPU output plane stride & plane offset values are "tightly
	 * packed", that is, they do not contain extra room for padding bytes. If
	 * they do, and downstream can't handle videometas, then frames have to
	 * be copied into a form that *is* tightly packed, otherwise downstream
	 * will get frames with padding bytes and not know that these need to be
	 * skipped. */
	gboolean need_to_copy_output_frames;

	/* Decoding thread states. */
	GThread *decoding_thread;
	GstImxVpuDecDecodingThreadState decoding_thread_state;
	GMutex decoding_thread_mutex;
	GCond decoding_thread_cond;

	/* A small queue (max. 3 items) that contains frames to be decoded.
	 * Frames are queued into it in gst_imx_vpu_dec_handle_frame(), and
	 * dequeued in gst_imx_vpu_dec_dequeue_and_push_frame(). */
	GQueue *encoded_frame_queue;
};


struct _GstImxVpuDecClass
{
	GstVideoDecoderClass parent_class;

	gboolean (*is_frame_reordering_required)(GstStructure *format);

	/* This is a copy of the flag from the entry from the
	 * GstImxVpuCodecDetails table that corresponds to the compression
	 * format the decoder handles. */
	gboolean requires_codec_data;
};


G_DEFINE_ABSTRACT_TYPE(GstImxVpuDec, gst_imx_vpu_dec, GST_TYPE_VIDEO_DECODER)


static void gst_imx_vpu_dec_finalize(GObject *object);

static GstStateChangeReturn gst_imx_vpu_dec_change_state(GstElement *element, GstStateChange transition);

static gboolean gst_imx_vpu_dec_start(GstVideoDecoder *decoder);
static gboolean gst_imx_vpu_dec_stop(GstVideoDecoder *decoder);
static gboolean gst_imx_vpu_dec_set_format(GstVideoDecoder *decoder, GstVideoCodecState *state);
static GstFlowReturn gst_imx_vpu_dec_handle_frame(GstVideoDecoder *decoder, GstVideoCodecFrame *cur_frame);
static gboolean gst_imx_vpu_dec_flush(GstVideoDecoder *decoder);
static GstFlowReturn gst_imx_vpu_dec_drain(GstVideoDecoder *decoder);
static GstFlowReturn gst_imx_vpu_dec_finish(GstVideoDecoder *decoder);
static gboolean gst_imx_vpu_dec_decide_allocation(GstVideoDecoder *decoder, GstQuery *query);
static gboolean gst_imx_vpu_dec_sink_event(GstVideoDecoder *decoder, GstEvent *event);

static void gst_imx_vpu_dec_start_decoding_thread(GstImxVpuDec *imx_vpu_dec);
static gboolean gst_imx_vpu_dec_drain_decoding_thread(GstImxVpuDec *imx_vpu_dec);
static void gst_imx_vpu_dec_stop_decoding_thread(GstImxVpuDec *imx_vpu_dec, gboolean release_stream_lock_before_joining);
static gpointer gst_imx_vpu_dec_decoding_thread(gpointer user_data);

static gboolean gst_imx_vpu_dec_dequeue_and_push_frame(GstImxVpuDec *imx_vpu_dec);
static GstFlowReturn gst_imx_vpu_dec_decode_queued_frames(GstImxVpuDec *imx_vpu_dec);
static void gst_imx_vpu_dec_teardown_current_decoder(GstImxVpuDec *imx_vpu_dec);
static void gst_imx_vpu_dec_unref_decoder_context(GstImxVpuDec *imx_vpu_dec);
static gboolean gst_imx_vpu_dec_allocate_and_add_framebuffers(GstImxVpuDec *imx_vpu_dec, size_t num_framebuffers);
static GstFlowReturn gst_imx_vpu_dec_copy_output_frame_if_needed(GstImxVpuDec *imx_vpu_dec, GstVideoCodecFrame *output_frame);


static void gst_imx_vpu_dec_class_init(GstImxVpuDecClass *klass)
{
	GObjectClass *object_class;
	GstElementClass *element_class;
	GstVideoDecoderClass *video_decoder_class;

	gst_imx_vpu_api_setup_logging();

	GST_DEBUG_CATEGORY_INIT(imx_vpu_dec_debug, "imxvpudec", 0, "NXP i.MX VPU video decoder");

	object_class = G_OBJECT_CLASS(klass);
	element_class = GST_ELEMENT_CLASS(klass);
	video_decoder_class = GST_VIDEO_DECODER_CLASS(klass);

	object_class->finalize                 = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_finalize);
	element_class->change_state            = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_change_state);
	video_decoder_class->start             = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_start);
	video_decoder_class->stop              = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_stop);
	video_decoder_class->set_format        = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_set_format);
	video_decoder_class->handle_frame      = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_handle_frame);
	video_decoder_class->flush             = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_flush);
	video_decoder_class->drain             = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_drain);
	video_decoder_class->finish            = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_finish);
	video_decoder_class->decide_allocation = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_decide_allocation);
	video_decoder_class->sink_event        = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_sink_event);

	klass->is_frame_reordering_required = NULL;
	klass->requires_codec_data = FALSE;
}


static void gst_imx_vpu_dec_init(GstImxVpuDec *imx_vpu_dec)
{
	imx_vpu_dec->codec_data = NULL;
	imx_vpu_dec->codec_data_is_mapped = FALSE;

	imx_vpu_dec->input_state = NULL;
	imx_vpu_dec->output_state = NULL;

	imx_vpu_dec->decoder_context = NULL;
	imx_vpu_dec->dma_buffer_pool = NULL;
	imx_vpu_dec->nonvideometa_output_buffer_pool = NULL;
	imx_vpu_dec->prepared_output_buffer = NULL;

	imx_vpu_dec->stream_buffer = NULL;
	imx_vpu_dec->decoder = NULL;
	imx_vpu_dec->dec_global_info = imx_vpu_api_dec_get_global_info();
	memset(&(imx_vpu_dec->open_params), 0, sizeof(imx_vpu_dec->open_params));
	imx_vpu_dec->default_dma_buf_allocator = NULL;

	imx_vpu_dec->fatal_error_cannot_decode = FALSE;

	imx_vpu_dec->decoding_thread = NULL;
	imx_vpu_dec->decoding_thread_state = GST_IMX_VPU_DEC_DECODING_THREAD_STATE_INACTIVE;
	g_mutex_init(&(imx_vpu_dec->decoding_thread_mutex));
	g_cond_init(&(imx_vpu_dec->decoding_thread_cond));
	imx_vpu_dec->encoded_frame_queue = g_queue_new();
}


static void gst_imx_vpu_dec_finalize(GObject *object)
{
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC(object);

	g_mutex_clear(&(imx_vpu_dec->decoding_thread_mutex));
	g_cond_clear(&(imx_vpu_dec->decoding_thread_cond));
	g_queue_free(imx_vpu_dec->encoded_frame_queue);

	G_OBJECT_CLASS(gst_imx_vpu_dec_parent_class)->finalize(object);
}


static GstStateChangeReturn gst_imx_vpu_dec_change_state(GstElement *element, GstStateChange transition)
{
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC(element);

	switch (transition)
	{
		case GST_STATE_CHANGE_PAUSED_TO_READY:
			if (imx_vpu_dec->decoder_context != NULL)
			{
				/* Change to the STOPPING state to force the decoding
				* thread loop to immediately exit. */
				DECODING_THREAD_LOCK(imx_vpu_dec);
				imx_vpu_dec->decoding_thread_state = GST_IMX_VPU_DEC_DECODING_THREAD_STATE_STOPPING;
				DECODING_THREAD_SIGNAL(imx_vpu_dec);
				DECODING_THREAD_UNLOCK(imx_vpu_dec);
			}
			break;
		default:
			break;
	}

	return GST_ELEMENT_CLASS(gst_imx_vpu_dec_parent_class)->change_state (element, transition);
}


static gboolean gst_imx_vpu_dec_start(GstVideoDecoder *decoder)
{
	gboolean ret = TRUE;
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC(decoder);
	size_t stream_buffer_size, stream_buffer_alignment;
	GstAllocationParams alloc_params;
	ImxVpuApiCompressionFormat compression_format = GST_IMX_VPU_GET_ELEMENT_COMPRESSION_FORMAT(decoder);
	GstImxVpuCodecDetails const * codec_details = gst_imx_vpu_get_codec_details(compression_format);

	imx_vpu_dec->fatal_error_cannot_decode = FALSE;


	/* Set up the stream buffer */

	stream_buffer_size = imx_vpu_dec->dec_global_info->min_required_stream_buffer_size;
	stream_buffer_alignment = imx_vpu_dec->dec_global_info->required_stream_buffer_physaddr_alignment;

	GST_DEBUG_OBJECT(
		imx_vpu_dec,
		"stream buffer info:  required min size: %zu bytes  required alignment: %zu  decoded frames are from buffer pool: %d",
		stream_buffer_size,
		stream_buffer_alignment,
		!!(imx_vpu_dec->dec_global_info->flags & IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_DECODED_FRAMES_ARE_FROM_BUFFER_POOL)
	);

	memset(&alloc_params, 0, sizeof(alloc_params));
	alloc_params.align = stream_buffer_alignment - 1;

	imx_vpu_dec->default_dma_buf_allocator = gst_imx_allocator_new();

	if (stream_buffer_size > 0)
	{
		imx_vpu_dec->stream_buffer = gst_allocator_alloc(
			imx_vpu_dec->default_dma_buf_allocator,
			stream_buffer_size,
			&alloc_params
		);
		if (G_UNLIKELY(imx_vpu_dec->stream_buffer == NULL))
		{
			GST_ELEMENT_ERROR(imx_vpu_dec, RESOURCE, FAILED, ("could not allocate DMA memory for stream buffer"), (NULL));
			ret = FALSE;
			goto finish;
		}
	}
	else
		GST_DEBUG_OBJECT(imx_vpu_dec, "not allocating stream buffer since the VPU does not need one");


	/* VPU decoder setup continues in set_format(), since we need to
	 * know the input caps to fill the open_params structure. */

	GST_INFO_OBJECT(imx_vpu_dec, "i.MX VPU %s decoder started", codec_details->desc_name);


finish:
	return ret;
}


static gboolean gst_imx_vpu_dec_stop(GstVideoDecoder *decoder)
{
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC(decoder);
	ImxVpuApiCompressionFormat compression_format = GST_IMX_VPU_GET_ELEMENT_COMPRESSION_FORMAT(decoder);
	GstImxVpuCodecDetails const * codec_details = gst_imx_vpu_get_codec_details(compression_format);

	/* Immediately stop the decoding thread. Any queued
	 * but not yet decoded frames are discarded. */
	gst_imx_vpu_dec_stop_decoding_thread(imx_vpu_dec, FALSE);
	gst_imx_vpu_dec_teardown_current_decoder(imx_vpu_dec);

	if (imx_vpu_dec->stream_buffer != NULL)
	{
		gst_memory_unref(imx_vpu_dec->stream_buffer);
		imx_vpu_dec->stream_buffer = NULL;
	}

	if (imx_vpu_dec->default_dma_buf_allocator != NULL)
	{
		gst_object_unref(GST_OBJECT(imx_vpu_dec->default_dma_buf_allocator));
		imx_vpu_dec->default_dma_buf_allocator = NULL;
	}

	GST_INFO_OBJECT(imx_vpu_dec, "i.MX VPU %s decoder stopped", codec_details->desc_name);

	return TRUE;
}


static gboolean gst_imx_vpu_dec_set_format(GstVideoDecoder *decoder, GstVideoCodecState *state)
{
	ImxVpuApiDecReturnCodes dec_ret;
	GstVideoFormat downstream_format;
	gboolean ret = TRUE;
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC_CAST(decoder);
	GstImxVpuDecClass *klass = GST_IMX_VPU_DEC_CLASS(G_OBJECT_GET_CLASS(decoder));
	ImxVpuApiDecGlobalInfo const *dec_global_info = imx_vpu_dec->dec_global_info;
	ImxVpuApiDecOpenParams *open_params = &(imx_vpu_dec->open_params);
	GstCaps *allowed_srccaps = NULL;
	ImxVpuApiCompressionFormat compression_format = GST_IMX_VPU_GET_ELEMENT_COMPRESSION_FORMAT(decoder);

	GST_DEBUG_OBJECT(decoder, "setting decoder format");


	/* Drain frames that are already decoded but not yet displayed. */
	ret = gst_imx_vpu_dec_drain_decoding_thread(imx_vpu_dec);
	gst_imx_vpu_dec_stop_decoding_thread(imx_vpu_dec, TRUE);
	if (!ret)
	{
		imx_vpu_dec->fatal_error_cannot_decode = TRUE;
		GST_ERROR_OBJECT(imx_vpu_dec, "cannot set new format: draining existing decoder failed");
		goto finish;
	}


	/* Cleanup any existing data and states. */
	gst_imx_vpu_dec_teardown_current_decoder(imx_vpu_dec);


	/* Get the caps that downstream allows. Amongst other things, this allows
	 * us to pick a video format that is suitable for decoding. */
	{
		gchar const *format_str;
		GValue const *format_value;
		downstream_format = GST_VIDEO_FORMAT_UNKNOWN;

		allowed_srccaps = gst_pad_get_allowed_caps(GST_VIDEO_DECODER_SRC_PAD(decoder));

		if (allowed_srccaps != NULL)
		{
			GST_DEBUG_OBJECT(imx_vpu_dec, "allowed srcccaps: %" GST_PTR_FORMAT, (gpointer)allowed_srccaps);

			/* Look at the sample format values from the first structure */
			GstStructure *structure = gst_caps_get_structure(allowed_srccaps, 0);
			format_value = gst_structure_get_value(structure, "format");

			if (format_value == NULL)
			{
				ret = FALSE;
				goto finish;
			}
			else if (GST_VALUE_HOLDS_LIST(format_value))
			{
				/* if value is a format list, pick the first entry */
				GValue const *fmt_list_value = gst_value_list_get_value(format_value, 0);
				format_str = g_value_get_string(fmt_list_value);
			}
			else if (G_VALUE_HOLDS_STRING(format_value))
			{
				/* if value is a string, use it directly */
				format_str = g_value_get_string(format_value);
			}
			else
			{
				GST_ERROR_OBJECT(imx_vpu_dec, "unexpected type for 'format' field in allowed_srccaps structure %" GST_PTR_FORMAT, (gpointer)structure);
				ret = FALSE;
				goto finish;
			}

			downstream_format = gst_video_format_from_string(format_str);
			g_assert(downstream_format != GST_VIDEO_FORMAT_UNKNOWN);
		}
	}


	/* Begin filling the open_params. */

	memset(open_params, 0, sizeof(ImxVpuApiDecOpenParams));

	/* Get the codec data if required. */
	if (klass->requires_codec_data)
	{
		GValue const *value;
		GstStructure *structure;

		structure = gst_caps_get_structure(state->caps, 0);

		value = gst_structure_get_value(structure, "codec_data");
		if (value != NULL)
		{
			GstBuffer *codec_data;
			GST_DEBUG_OBJECT(imx_vpu_dec, "codec data expected and found in caps");
			codec_data = gst_value_get_buffer(value);

			/* Copy the codec data buffer, to make sure the codec_data
	 		 * lifetime does not depend on the caps. */
			imx_vpu_dec->codec_data = gst_buffer_copy(codec_data);
		}
		else
		{
			GST_ERROR_OBJECT(imx_vpu_dec, "codec data expected, but not found in caps");
			ret = FALSE;
			goto finish;
		}
	}

	open_params->frame_width = state->info.width;
	open_params->frame_height = state->info.height;
	open_params->compression_format = compression_format;

	/* By default, we want the VPU to reorder the frames. We can keep track of
	 * this reordering through the GstVideoDecoder system frame numbers. In
	 * some cases though, it may be beneficial to disable it. It may lower
	 * latency to turn it off if it is not necessary, for example. */
	if ((klass->is_frame_reordering_required == NULL) || klass->is_frame_reordering_required(gst_caps_get_structure(state->caps, 0)))
	{
		GST_DEBUG_OBJECT(imx_vpu_dec, "using frame reodering");
		open_params->flags |= IMX_VPU_API_DEC_OPEN_PARAMS_FLAG_ENABLE_FRAME_REORDERING;
	}
	else
		GST_DEBUG_OBJECT(imx_vpu_dec, "not using frame reodering");


	/* Check if 10-bit decoding is required. */

	if (allowed_srccaps == NULL)
	{
		open_params->flags |= IMX_VPU_API_DEC_OPEN_PARAMS_FLAG_USE_10BIT_DECODING;
		GST_DEBUG_OBJECT(imx_vpu_dec, "srcpad not linked (yet), so no src caps set; enabling 10-bit decoding by default");
	}
	else if (gst_imx_vpu_color_format_has_10bit(downstream_format))
	{
		open_params->flags |= IMX_VPU_API_DEC_OPEN_PARAMS_FLAG_USE_10BIT_DECODING;
		GST_DEBUG_OBJECT(imx_vpu_dec, "format %s detected in list of supported srccaps formats; enabling 10-bit decoding", gst_video_format_to_string(downstream_format));
	}


	/* Check if fully planar or semi planar frames shall be produced. */

	if ((dec_global_info->flags & (IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_FULLY_PLANAR_FRAMES_SUPPORTED | IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_SEMI_PLANAR_FRAMES_SUPPORTED)) == (IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_FULLY_PLANAR_FRAMES_SUPPORTED | IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_SEMI_PLANAR_FRAMES_SUPPORTED))
	{
		/* Find out what formats downstream supports, to determine
		 * if we have to request fully planar or semi planar data. */

		if (allowed_srccaps == NULL)
		{
			open_params->flags |= IMX_VPU_API_DEC_OPEN_PARAMS_FLAG_USE_SEMI_PLANAR_COLOR_FORMAT;
			GST_DEBUG_OBJECT(imx_vpu_dec, "srcpad not linked (yet), so no src caps set; enabling semi planar data by default");
		}
		else
		{
			gboolean is_semi_planar = gst_imx_vpu_color_format_is_semi_planar(downstream_format);
			if (is_semi_planar)
				open_params->flags |= IMX_VPU_API_DEC_OPEN_PARAMS_FLAG_USE_SEMI_PLANAR_COLOR_FORMAT;

			GST_DEBUG_OBJECT(imx_vpu_dec, "format %s detected in list of supported srccaps formats; setting semi planar data flag in open params to %d", gst_video_format_to_string(downstream_format), is_semi_planar);
		}
	}
	else if (dec_global_info->flags & IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_SEMI_PLANAR_FRAMES_SUPPORTED)
	{
		GST_DEBUG_OBJECT(imx_vpu_dec, "decoder only supports semi planar formats");
		open_params->flags |= IMX_VPU_API_DEC_OPEN_PARAMS_FLAG_USE_SEMI_PLANAR_COLOR_FORMAT;
	}
	else if (dec_global_info->flags & IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_FULLY_PLANAR_FRAMES_SUPPORTED)
	{
		GST_DEBUG_OBJECT(imx_vpu_dec, "decoder only supports fully planar formats");
	}


	/* Map the codec data since the decoder needs to be able
	 * to access it in the dequeue_and_push_frame() calls. */
	if (imx_vpu_dec->codec_data != NULL)
	{
		gst_buffer_map(imx_vpu_dec->codec_data, &(imx_vpu_dec->codecdata_map_info), GST_MAP_READ);
		imx_vpu_dec->codec_data_is_mapped = TRUE;

		/* The decoder does _not_ copy the bytes from codec_data. Instead, it
		 * just stores a pointer to the codec data, which is why we must keep
		 * codec_data mapped, and why codec_data must exist at least for the
		 * duration of the decoding. */
		GST_LOG_OBJECT(imx_vpu_dec, "using extra codec data (%" G_GSIZE_FORMAT " byte) from gstbuffer %p", imx_vpu_dec->codecdata_map_info.size, (gpointer)(imx_vpu_dec->codec_data));
		open_params->extra_header_data = imx_vpu_dec->codecdata_map_info.data;
		open_params->extra_header_data_size = imx_vpu_dec->codecdata_map_info.size;
	}


	/* open_params filled with valid data. Now we can actually open a new VPU
	 * decoder instance, as a contination from what we began in start().
	 * (The instance is closed in gst_imx_vpu_dec_context_close_decoder() ). */
	GST_DEBUG_OBJECT(decoder, "(re)opening decoder");
	if ((dec_ret = imx_vpu_api_dec_open(
		&(imx_vpu_dec->decoder),
		open_params,
		(imx_vpu_dec->stream_buffer != NULL) ? gst_imx_get_dma_buffer_from_memory(imx_vpu_dec->stream_buffer) : NULL
	)) != IMX_VPU_API_DEC_RETURN_CODE_OK)
	{
		GST_ERROR_OBJECT(imx_vpu_dec, "could not open decoder: %s", imx_vpu_api_dec_return_code_string(dec_ret));
		ret = FALSE;
		goto finish;
	}

	/* Create new context for the decoder. */
	imx_vpu_dec->decoder_context = gst_imx_vpu_dec_context_new(imx_vpu_dec->decoder);
	gst_object_ref_sink(GST_OBJECT_CAST(imx_vpu_dec->decoder_context));
	g_assert(imx_vpu_dec->decoder_context != NULL);

	/* Ref the codec state, to be able to use it later as reference
	 * for the gst_video_decoder_set_output_state() function. */
	imx_vpu_dec->input_state = gst_video_codec_state_ref(state);


	GST_DEBUG_OBJECT(decoder, "setting format finished");


	/* The decoder is set up, decoding can begin. */
	gst_imx_vpu_dec_start_decoding_thread(imx_vpu_dec);


finish:
	if (allowed_srccaps != NULL)
		gst_caps_unref(allowed_srccaps);

	return ret;
}


static GstFlowReturn gst_imx_vpu_dec_handle_frame(GstVideoDecoder *decoder, GstVideoCodecFrame *cur_frame)
{
	/* In here, we queue cur_frame into encoded_frame_queue. The actual
	 * decoding happens in gst_imx_vpu_dec_decoding_thread(). */

	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC_CAST(decoder);
	GstFlowReturn flow_ret;

	/* Sanity checks. */

	if (G_UNLIKELY(imx_vpu_dec->decoder == NULL))
	{
		GST_ERROR_OBJECT(imx_vpu_dec, "decoder was not initialized; cannot continue");
		gst_video_codec_frame_unref(cur_frame);
		imx_vpu_dec->fatal_error_cannot_decode = TRUE;
		return GST_FLOW_ERROR;
	}

	if (G_UNLIKELY(imx_vpu_dec->fatal_error_cannot_decode))
	{
		GST_ERROR_OBJECT(imx_vpu_dec, "fatal error previously recorded; cannot decode");
		gst_video_codec_frame_unref(cur_frame);
		return GST_FLOW_ERROR;
	}

	if (G_UNLIKELY(cur_frame == NULL))
		return GST_FLOW_OK;

	flow_ret = GST_FLOW_OK;

	DECODING_THREAD_LOCK(imx_vpu_dec);

	while (TRUE)
	{
		/* We are only supposed to queue frames in the RUNNING state. */

		if (imx_vpu_dec->decoding_thread_state == GST_IMX_VPU_DEC_DECODING_THREAD_STATE_RUNNING)
		{
			if (g_queue_get_length(imx_vpu_dec->encoded_frame_queue) > 2)
			{
				GST_LOG_OBJECT(imx_vpu_dec, "encoded frame queue is full; waiting until there is free space");

				GST_VIDEO_DECODER_STREAM_UNLOCK(imx_vpu_dec);
				DECODING_THREAD_WAIT(imx_vpu_dec);
				GST_VIDEO_DECODER_STREAM_LOCK(imx_vpu_dec);
			}
			else
			{
				GST_LOG_OBJECT(imx_vpu_dec, "encoded frame queue is not full; pushing encoded frame into it");
				g_queue_push_head(imx_vpu_dec->encoded_frame_queue, cur_frame);
				DECODING_THREAD_SIGNAL(imx_vpu_dec);
				break;
			}
		}
		else
		{
			if (imx_vpu_dec->decoding_thread_state == GST_IMX_VPU_DEC_DECODING_THREAD_STATE_FAILED)
			{
				imx_vpu_dec->fatal_error_cannot_decode = TRUE;
				flow_ret = GST_FLOW_ERROR;
			}
			else if (imx_vpu_dec->decoding_thread_state == GST_IMX_VPU_DEC_DECODING_THREAD_STATE_STOPPING || imx_vpu_dec->decoding_thread_state == GST_IMX_VPU_DEC_DECODING_THREAD_STATE_INACTIVE)
			{
				flow_ret = GST_FLOW_FLUSHING;
			}

			gst_video_codec_frame_unref(cur_frame);
			break;
		}
	}

	DECODING_THREAD_UNLOCK(imx_vpu_dec);

	return flow_ret;
}


static gboolean gst_imx_vpu_dec_flush(GstVideoDecoder *decoder)
{
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC_CAST(decoder);

	if (imx_vpu_dec->decoder == NULL)
		return TRUE;

	GST_DEBUG_OBJECT(imx_vpu_dec, "flushing decoder");

	/* decoder_context == NULL may happen with single-frame
	 * decoding like with WebP data or JPEG pictures. */
	if (imx_vpu_dec->decoder_context != NULL)
	{
		/* Stop the thread. This immediately stop the ongoing decoding
		 * and discards any queued and not yet decoded frames. */
		gst_imx_vpu_dec_stop_decoding_thread(imx_vpu_dec, TRUE);

		GST_IMX_VPU_DEC_CONTEXT_LOCK(imx_vpu_dec->decoder_context);
		imx_vpu_api_dec_flush(imx_vpu_dec->decoder);
		GST_IMX_VPU_DEC_CONTEXT_UNLOCK(imx_vpu_dec->decoder_context);

		if (imx_vpu_dec->decoding_thread_state == GST_IMX_VPU_DEC_DECODING_THREAD_STATE_FAILED)
		{
			imx_vpu_dec->fatal_error_cannot_decode = TRUE;
			return FALSE;
		}

		/* Restart the thread after the decoder was fully flushed. */
		gst_imx_vpu_dec_start_decoding_thread(imx_vpu_dec);
	}
	else
		imx_vpu_api_dec_flush(imx_vpu_dec->decoder);

	return TRUE;
}


static GstFlowReturn gst_imx_vpu_dec_drain(GstVideoDecoder *decoder)
{
	GstFlowReturn flow_ret;
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC_CAST(decoder);

	if (imx_vpu_dec->decoder == NULL)
		return GST_FLOW_OK;

	if (G_UNLIKELY(imx_vpu_dec->fatal_error_cannot_decode))
		return GST_FLOW_OK;

	GST_INFO_OBJECT(imx_vpu_dec, "draining decoder");

	GST_VIDEO_DECODER_STREAM_UNLOCK(imx_vpu_dec);
	flow_ret = gst_imx_vpu_dec_drain_decoding_thread(imx_vpu_dec) ? GST_FLOW_OK : GST_FLOW_ERROR;
	GST_VIDEO_DECODER_STREAM_LOCK(imx_vpu_dec);

	if (G_UNLIKELY(flow_ret == GST_FLOW_ERROR))
		imx_vpu_dec->fatal_error_cannot_decode = TRUE;

	return flow_ret;
}


static GstFlowReturn gst_imx_vpu_dec_finish(GstVideoDecoder *decoder)
{
	return gst_imx_vpu_dec_drain(decoder);
}


static gboolean gst_imx_vpu_dec_decide_allocation(GstVideoDecoder *decoder, GstQuery *query)
{
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC_CAST(decoder);
	GstBufferPool *buffer_pool = NULL;
	GstStructure *pool_config;
	guint buffer_size, pool_index, plane_index;
	GstCaps *negotiated_caps;
	GstVideoInfo negotiated_video_info;
	GstVideoInfo const *dma_bufpool_video_info;
	gboolean vpu_output_buffers_are_tightly_packed;
	gboolean downstream_supports_video_meta;
	guint video_meta_index;
	GstImxDmaBufferAllocator *imx_dma_buffer_allocator = NULL;

	/* This happens if gap events are sent downstream before the first caps event.
	 * GstVideoDecoder then produces default sink caps and negotiates with these
	 * caps, which ultimately ends up calling decide_allocation() even though there
	 * is no decoder instance yet. */
	if (G_UNLIKELY(imx_vpu_dec->decoder == NULL))
	{
		gst_query_parse_allocation(query, &negotiated_caps, NULL);
		GST_WARNING_OBJECT(
			imx_vpu_dec,
			"not responding to allocation query since no decoder exists (yet); negotiated caps = %" GST_PTR_FORMAT,
			(gpointer)negotiated_caps
		);

		return FALSE;
	}

	GST_DEBUG_OBJECT(imx_vpu_dec, "can respond to allocation query since decoder exists");


	/* Create and set up a DMA buffer pool if one does not exist already. */
	if (imx_vpu_dec->dma_buffer_pool == NULL)
	{
		GST_DEBUG_OBJECT(imx_vpu_dec, "no DMA buffer pool exists yet; creating one, and trying to use any i.MX DMA buffer allocator present in the query");

		gst_video_info_init(&negotiated_video_info);
		gst_query_parse_allocation(query, &negotiated_caps, NULL);
		if (negotiated_caps != NULL)
		{
			GST_DEBUG_OBJECT(imx_vpu_dec, "negotiated caps in allocation query: %" GST_PTR_FORMAT, (gpointer)negotiated_caps);
			if (G_UNLIKELY(!gst_video_info_from_caps(&negotiated_video_info, negotiated_caps)))
			{
				GST_ERROR_OBJECT(imx_vpu_dec, "caps cannot be converted to a video info structure");
				return FALSE;
			}
		}

		GST_DEBUG_OBJECT(decoder, "number of allocation buffer pools in query: %d", gst_query_get_n_allocation_pools(query));

		/* See if downstream supports video meta. */
		downstream_supports_video_meta = gst_query_find_allocation_meta(query, GST_VIDEO_META_API_TYPE, &video_meta_index);
		GST_DEBUG_OBJECT(imx_vpu_dec, "video meta supported by downstream: %d", downstream_supports_video_meta);

		buffer_size = MAX(negotiated_video_info.size, imx_vpu_dec->current_stream_info.min_fb_pool_framebuffer_size);

		/* Iterate over all pools and look for one that has an allocator
		 * which implements the GstImxDmaBufferAllocator interface. */
		for (pool_index = 0; pool_index < gst_query_get_n_allocation_pools(query); ++pool_index)
		{
			GstAllocator *candidate_allocator;
			GstBufferPool *current_buffer_pool;
			gboolean is_imx_dma_buffer_allocator = FALSE;

			gst_query_parse_nth_allocation_pool(query, pool_index, &current_buffer_pool, NULL, NULL, NULL);

			pool_config = gst_buffer_pool_get_config(current_buffer_pool);
			if (gst_buffer_pool_config_get_allocator(pool_config, &candidate_allocator, NULL))
				is_imx_dma_buffer_allocator = GST_IS_IMX_DMA_BUFFER_ALLOCATOR(candidate_allocator);

			GST_DEBUG_OBJECT(imx_vpu_dec, "buffer pool %p (#%u in allocation query) has an i.MX DMA buffer allocator: %d", (gpointer)current_buffer_pool, pool_index, is_imx_dma_buffer_allocator);

			gst_structure_free(pool_config);
			gst_object_unref(GST_OBJECT(current_buffer_pool));

			if (is_imx_dma_buffer_allocator)
			{
				imx_dma_buffer_allocator = GST_IMX_DMA_BUFFER_ALLOCATOR(candidate_allocator);
				break;
			}
		}

		/* If no suitable allocator was found, use ours. */
		if (imx_dma_buffer_allocator == NULL)
		{
			GST_DEBUG_OBJECT(imx_vpu_dec, "no buffer pool in the allocation query has an i.MX DMA buffer allocator; using the default one");
			imx_dma_buffer_allocator = GST_IMX_DMA_BUFFER_ALLOCATOR(imx_vpu_dec->default_dma_buf_allocator);
		}

		/* Now create our DMA buffer pool. */
		imx_vpu_dec->dma_buffer_pool = gst_imx_vpu_dec_buffer_pool_new(&(imx_vpu_dec->current_stream_info), imx_vpu_dec->decoder_context);
		buffer_pool = GST_BUFFER_POOL(imx_vpu_dec->dma_buffer_pool);

		/* And configure our newly created pool. */
		pool_config = gst_buffer_pool_get_config(buffer_pool);
		gst_buffer_pool_config_set_params(pool_config, negotiated_caps, buffer_size, 0, 0);
		gst_buffer_pool_config_set_allocator(pool_config, GST_ALLOCATOR(imx_dma_buffer_allocator), NULL);
		if (downstream_supports_video_meta)
			gst_buffer_pool_config_add_option(pool_config, GST_BUFFER_POOL_OPTION_VIDEO_META);
		gst_buffer_pool_config_add_option(pool_config, GST_BUFFER_POOL_OPTION_IMX_VPU_DEC_BUFFER_POOL);
		gst_buffer_pool_set_config(buffer_pool, pool_config);

		/* Check if the plane stride & plane offset values are "tightly packed".
		 * See the vpu_output_buffers_are_tightly_packed for more details. */
		dma_bufpool_video_info = gst_imx_vpu_dec_buffer_pool_get_video_info(imx_vpu_dec->dma_buffer_pool);
		vpu_output_buffers_are_tightly_packed = TRUE;
		for (plane_index = 0; plane_index < GST_VIDEO_INFO_N_PLANES(dma_bufpool_video_info); ++plane_index)
		{
			gint negotiated_info_stride = GST_VIDEO_INFO_PLANE_STRIDE(&negotiated_video_info, plane_index);
			gint dma_bufpool_info_stride = GST_VIDEO_INFO_PLANE_STRIDE(dma_bufpool_video_info, plane_index);

			if (negotiated_info_stride != dma_bufpool_info_stride)
			{
				GST_DEBUG_OBJECT(
					imx_vpu_dec,
					"found difference in stride values for plane #%d:  negotiated video info stride: %d  DMA buffer pool video info stride: %d",
					plane_index,
					negotiated_info_stride,
					dma_bufpool_info_stride
				);

				vpu_output_buffers_are_tightly_packed = FALSE;
				break;
			}
		}
		GST_DEBUG_OBJECT(imx_vpu_dec, "VPU output buffers are tightly packed: %d", vpu_output_buffers_are_tightly_packed);

		/* If the plane stride & plane offset values aren't "tightly packed",
		 * _and_ downstream does not support meta, then we must copy the VPU's
		 * output frames into a form that _is_ "tightly packed", because without
		 * video meta, downstream cannot know which parts of the frame contain
		 * actual pixels and which ones contain padding bytes. */
		imx_vpu_dec->need_to_copy_output_frames = !(downstream_supports_video_meta || vpu_output_buffers_are_tightly_packed);

		GST_DEBUG_OBJECT(imx_vpu_dec, "need to copy output frames: %d", imx_vpu_dec->need_to_copy_output_frames);

		if (imx_vpu_dec->need_to_copy_output_frames)
		{
			/* Create "nonvideometa" buffer pool. We need this if frames
			 * copies are necessary, because the final output buffers
			 * will come from this very buffer pool. */

			GstAllocationParams allocation_params;

			buffer_size = GST_VIDEO_INFO_SIZE(&negotiated_video_info);

			gst_allocation_params_init(&allocation_params);

			imx_vpu_dec->nonvideometa_output_buffer_pool = gst_video_buffer_pool_new();

			memcpy(&(imx_vpu_dec->nonvideometa_output_video_info), &negotiated_video_info, sizeof(GstVideoInfo));

			pool_config = gst_buffer_pool_get_config(imx_vpu_dec->nonvideometa_output_buffer_pool);
			gst_buffer_pool_config_set_params(pool_config, negotiated_caps, buffer_size, 0, 0);
			gst_buffer_pool_config_set_allocator(pool_config, NULL, &allocation_params);
			gst_buffer_pool_set_config(imx_vpu_dec->nonvideometa_output_buffer_pool, pool_config);

			gst_buffer_pool_set_active(imx_vpu_dec->nonvideometa_output_buffer_pool, TRUE);

			GST_INFO_OBJECT(imx_vpu_dec, "need to copy VPU output frames since downstream cannot handle those directly; this may impact performance");
		}
	}
	else
	{
		GST_DEBUG_OBJECT(imx_vpu_dec, "DMA buffer pool exists already; continuing to use it");

		buffer_pool = GST_BUFFER_POOL(imx_vpu_dec->dma_buffer_pool);
		pool_config = gst_buffer_pool_get_config(buffer_pool);
		gst_buffer_pool_config_get_params(pool_config, NULL, &buffer_size, NULL, NULL);
		gst_structure_free(pool_config);
	}


	/* Now let the parent class do its processing. */
	if (!GST_VIDEO_DECODER_CLASS(gst_imx_vpu_dec_parent_class)->decide_allocation(decoder, query))
		return FALSE;


	/* Make sure the DMA buffer pool is picked by setting
	 * it as the first in the allocacation pool list. */
	if (gst_query_get_n_allocation_pools(query) == 0)
	{
		GST_DEBUG_OBJECT(imx_vpu_dec, "there are no allocation pools in the allocation query; adding DMA buffer pool to it");
		gst_query_add_allocation_pool(query, buffer_pool, buffer_size, 0, 0);
	}
	else
	{
		GST_DEBUG_OBJECT(imx_vpu_dec, "there are allocation pools in the allocation query; setting the DMA buffer pool as the first one in the query");
		gst_query_set_nth_allocation_pool(query, 0, buffer_pool, buffer_size, 0, 0);
	}


	return TRUE;
}

static gboolean gst_imx_vpu_dec_sink_event(GstVideoDecoder *decoder, GstEvent *event)
{
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC_CAST(decoder);
	gboolean ret;

	switch (GST_EVENT_TYPE(event))
	{
		case GST_EVENT_FLUSH_START:
			if (imx_vpu_dec->decoder_context != NULL)
			{
				/* Change to the STOPPING state to force the decoding
				* thread loop to immediately exit. */
				DECODING_THREAD_LOCK(imx_vpu_dec);
				imx_vpu_dec->decoding_thread_state = GST_IMX_VPU_DEC_DECODING_THREAD_STATE_STOPPING;
				DECODING_THREAD_SIGNAL(imx_vpu_dec);
				DECODING_THREAD_UNLOCK(imx_vpu_dec);
			}
			break;
		default:
			break;
	}

	ret = GST_VIDEO_DECODER_CLASS (gst_imx_vpu_dec_parent_class)->sink_event (decoder, event);

	return ret;
}

static void gst_imx_vpu_dec_start_decoding_thread(GstImxVpuDec *imx_vpu_dec)
{
	if (imx_vpu_dec->decoding_thread != NULL)
		return;

	imx_vpu_dec->decoding_thread_state = GST_IMX_VPU_DEC_DECODING_THREAD_STATE_RUNNING;
	imx_vpu_dec->decoding_thread = g_thread_new(
		"ImxVpuDec",
		gst_imx_vpu_dec_decoding_thread,
		(gpointer)imx_vpu_dec
	);
	g_assert(imx_vpu_dec->decoding_thread != NULL);
}


static gboolean gst_imx_vpu_dec_drain_decoding_thread(GstImxVpuDec *imx_vpu_dec)
{
	gboolean retval = TRUE;

	if (imx_vpu_dec->decoding_thread == NULL)
		return TRUE;

	DECODING_THREAD_LOCK(imx_vpu_dec);

	/* No point in draining if the current state isn't RUNNING. */
	if (imx_vpu_dec->decoding_thread_state != GST_IMX_VPU_DEC_DECODING_THREAD_STATE_RUNNING)
		goto finish;

	GST_DEBUG_OBJECT(imx_vpu_dec, "starting drain");

	/* Change the state to DRAINING and notify the decoding thread. */
	imx_vpu_dec->decoding_thread_state = GST_IMX_VPU_DEC_DECODING_THREAD_STATE_DRAINING;
	DECODING_THREAD_SIGNAL(imx_vpu_dec);

	GST_DEBUG_OBJECT(imx_vpu_dec, "waiting for drain to complete");

	/* Wait for draining to be completed. */
	while (imx_vpu_dec->decoding_thread_state == GST_IMX_VPU_DEC_DECODING_THREAD_STATE_DRAINING)
		DECODING_THREAD_WAIT(imx_vpu_dec);

	retval = (imx_vpu_dec->decoding_thread_state != GST_IMX_VPU_DEC_DECODING_THREAD_STATE_FAILED);

finish:
	DECODING_THREAD_UNLOCK(imx_vpu_dec);
	return retval;
}


static void gst_imx_vpu_dec_stop_decoding_thread(GstImxVpuDec *imx_vpu_dec, gboolean release_stream_lock_before_joining)
{
	if (imx_vpu_dec->decoding_thread == NULL)
		return;

	/* Change to the STOPPING state to force the decoding
	 * thread loop to immediately exit. */
	DECODING_THREAD_LOCK(imx_vpu_dec);
	imx_vpu_dec->decoding_thread_state = GST_IMX_VPU_DEC_DECODING_THREAD_STATE_STOPPING;
	DECODING_THREAD_SIGNAL(imx_vpu_dec);
	DECODING_THREAD_UNLOCK(imx_vpu_dec);

	/* Wait until the loop exits and the decoding thread function ends.
	 * If the stream lock is held, release it first. The decoding thread
	 * may be calling gst_imx_vpu_dec_decode_queued_frames(), which in
	 * turn may call gst_video_decoder_get_frame(), and that function
	 * takes that stream lock, leading to a deadlock if the stream lock
	 * was already held.
	 * This means that release_stream_lock_before_joining must be set to
	 * TRUE if the lock is held when gst_imx_vpu_dec_stop_decoding_thread()
	 * is called, and FALSE otherwise. */
	if (release_stream_lock_before_joining)
		GST_VIDEO_DECODER_STREAM_UNLOCK(imx_vpu_dec);
	g_thread_join(imx_vpu_dec->decoding_thread);
	if (release_stream_lock_before_joining)
		GST_VIDEO_DECODER_STREAM_LOCK(imx_vpu_dec);

	/* Thread is stopped. Discard it and reset the associated states. */
	g_thread_unref(imx_vpu_dec->decoding_thread);
	imx_vpu_dec->decoding_thread = NULL;
	imx_vpu_dec->decoding_thread_state = GST_IMX_VPU_DEC_DECODING_THREAD_STATE_INACTIVE;

	/* Unref the contents of encoded_frame_queue since these
	 * frames will not be decoded anymore. */
	while (!g_queue_is_empty(imx_vpu_dec->encoded_frame_queue))
	{
		GstVideoCodecFrame *gst_frame;

		gst_frame = g_queue_pop_tail(imx_vpu_dec->encoded_frame_queue);
		gst_video_codec_frame_unref(gst_frame);
	}
}


static gpointer gst_imx_vpu_dec_decoding_thread(gpointer user_data)
{
	gboolean keep_running = TRUE;
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC(user_data);

	while (keep_running)
	{
		GstFlowReturn flow_ret;
		gboolean skip_loop = FALSE;
		gboolean draining = FALSE;

		/* Lock the decoding thread mutex since we need to evaluate
		 * the current decoding state. */
		DECODING_THREAD_LOCK(imx_vpu_dec);

		switch (imx_vpu_dec->decoding_thread_state)
		{
			/* In the RUNNING state, we dequeue frames from the encoded_frame_queue and
			 * feed them to the decoder. If the encoded_frame_queue is empty, we wait
			 * until the queue is filled with frames or a decoding state change occurs. */
			case GST_IMX_VPU_DEC_DECODING_THREAD_STATE_RUNNING:
			{
				if (g_queue_is_empty(imx_vpu_dec->encoded_frame_queue))
				{
					DECODING_THREAD_WAIT(imx_vpu_dec);
					/* Skip the rest of the loop to reevaluate the current decoding thread state. */
					skip_loop = TRUE;
				}
				else
				{
					if (!gst_imx_vpu_dec_dequeue_and_push_frame(imx_vpu_dec))
					{
						/* An error occurred. We need to exit the loop. To inform the main
						 * streaming thread about the failure, we set the state to FAILED.
						 * Not signaling here since this will happen anyway below
						 * when leaving the loop. */
						keep_running = FALSE;
						imx_vpu_dec->decoding_thread_state = GST_IMX_VPU_DEC_DECODING_THREAD_STATE_FAILED;
					}
					else
					{
						if (imx_vpu_dec->decoding_thread_state != GST_IMX_VPU_DEC_DECODING_THREAD_STATE_RUNNING)
						{
							/* If the state changed during the gst_imx_vpu_dec_dequeue_and_push_frame()
							 * call, we skip the current loop to reevaluate the state. */
							skip_loop = TRUE;
						}
						else
						{
							/* Signal that there is now room in the encoded frame queue. */
							DECODING_THREAD_SIGNAL(imx_vpu_dec);
						}
					}
				}

				break;
			}

			/* In the DRAINING state, we first work through all frames in the encoded_frame_queue.
			 * Once that queue is empty, we enable the drain mode to force the decoder to decode
			 * any frames that may be in its own internal queue. */
			case GST_IMX_VPU_DEC_DECODING_THREAD_STATE_DRAINING:
			{
				if (g_queue_is_empty(imx_vpu_dec->encoded_frame_queue))
				{
					/* We worked through all the frames in encoded_frame_queue.
					 * Now enable the draining mode if not already done so. */
					if (!draining)
					{
						draining = TRUE;
						imx_vpu_api_dec_enable_drain_mode(imx_vpu_dec->decoder);
					}
				}
				else
				{
					/* We still need to process frames in the encoded_frame_queue
					 * before we can enable the drain mode. */
					if (!gst_imx_vpu_dec_dequeue_and_push_frame(imx_vpu_dec))
					{
						keep_running = FALSE;
						imx_vpu_dec->decoding_thread_state = GST_IMX_VPU_DEC_DECODING_THREAD_STATE_FAILED;
						/* An error occurred. We need to exit the loop. To inform the main
						 * streaming thread about the failure, we set the state to FAILED.
						 * Not signaling here since this will happen anyway below
						 * when leaving the loop. */
					}
					else
					{
						if (imx_vpu_dec->decoding_thread_state != GST_IMX_VPU_DEC_DECODING_THREAD_STATE_DRAINING)
						{
							/* If the state changed during the gst_imx_vpu_dec_dequeue_and_push_frame()
							 * call, we skip the current loop to reevaluate the state. */
							skip_loop = TRUE;
						}
					}
				}

				break;
			}

			/* In this state, the decoding loop is supposed to be stopped immediately. */
			case GST_IMX_VPU_DEC_DECODING_THREAD_STATE_STOPPING:
			{
				keep_running = FALSE;
				break;
			}

			/* Similarly to STOPPING, we must stop the loop immediately in this state. */
			case GST_IMX_VPU_DEC_DECODING_THREAD_STATE_FAILED:
			{
				keep_running = FALSE;
				break;
			}

			/* Should not occur.. */
			default:
				keep_running = FALSE;
				g_assert_not_reached();
				break;
		}

		DECODING_THREAD_UNLOCK(imx_vpu_dec);

		if (skip_loop)
		{
			GST_TRACE_OBJECT(imx_vpu_dec, "skipping loop");
			continue;
		}

		if (!keep_running)
		{
			GST_DEBUG_OBJECT(imx_vpu_dec, "stopping thread");
			break;
		}

		/* Perform the actual frame decoding. This function blocks until
		 * the VPU ran out of data to decode or an error occurred. In the
		 * DRAINING state, the VPU does not really "run out of data";
		 * rather, it just decodes all remaining data. Consequently,
		 * once this function call finishes successfully, we can consider
		 * the decoder to be fully drained. */
		flow_ret = gst_imx_vpu_dec_decode_queued_frames(imx_vpu_dec);

		switch (flow_ret)
		{
			case GST_FLOW_EOS:
				keep_running = FALSE;
				break;

			case GST_FLOW_OK:
				break;

			default:
				keep_running = FALSE;
				DECODING_THREAD_LOCK(imx_vpu_dec);
				imx_vpu_dec->decoding_thread_state = GST_IMX_VPU_DEC_DECODING_THREAD_STATE_FAILED;
				DECODING_THREAD_UNLOCK(imx_vpu_dec);
				break;
		}

		/* As mentioned above, in DRAINING mode, only one call to
		 * gst_imx_vpu_dec_decode_queued_frames() occurs, since one call
		 * is enough to fully drain the decoder. Unless an error occurred,
		 * we turn off the drain mode here and go back to business as usual. */
		if (draining)
		{
			DECODING_THREAD_LOCK(imx_vpu_dec);

			if (imx_vpu_dec->decoding_thread_state != GST_IMX_VPU_DEC_DECODING_THREAD_STATE_FAILED)
			{
				draining = FALSE;
				imx_vpu_api_dec_disable_drain_mode(imx_vpu_dec->decoder);
				imx_vpu_dec->decoding_thread_state = GST_IMX_VPU_DEC_DECODING_THREAD_STATE_RUNNING;
				GST_DEBUG_OBJECT(imx_vpu_dec, "drain complete");
				DECODING_THREAD_SIGNAL(imx_vpu_dec);
			}

			DECODING_THREAD_UNLOCK(imx_vpu_dec);
		}
	}

	DECODING_THREAD_SIGNAL(imx_vpu_dec);

	return NULL;
}


static gboolean gst_imx_vpu_dec_dequeue_and_push_frame(GstImxVpuDec *imx_vpu_dec)
{
	/* This must be called with the decoding thread mutex locked. */

	GstVideoCodecFrame *gst_frame;
	GstMapInfo in_map_info;
	ImxVpuApiEncodedFrame encoded_frame;
	ImxVpuApiDecReturnCodes dec_ret;
	gboolean retval = TRUE;

	g_assert(!g_queue_is_empty(imx_vpu_dec->encoded_frame_queue));
	gst_frame = g_queue_pop_tail(imx_vpu_dec->encoded_frame_queue);

	/* Unlock the mutex here since we won't touch the decoding state
	 * after this point until this function finishes. Since the
	 * imx_vpu_api_dec_push_encoded_frame() may block for a little
	 * while, we unlock the mutex to allow other threads to fill the
	 * encoded_frame_queue in the meantime. */
	DECODING_THREAD_UNLOCK(imx_vpu_dec);

	gst_buffer_map(gst_frame->input_buffer, &in_map_info, GST_MAP_READ);

	encoded_frame.data = in_map_info.data;
	encoded_frame.data_size = in_map_info.size;
	encoded_frame.pts = gst_frame->pts;
	encoded_frame.dts = gst_frame->dts;
	/* The system frame number is necessary to correctly associate encoded
	 * frames and decoded frames. This is required, because some formats
	 * have a delay (= output frames only show up after N complete input
	 * frames), and others like h.264 even reorder frames. */
	encoded_frame.context = (void *)((guintptr)(gst_frame->system_frame_number));

	GST_LOG_OBJECT(imx_vpu_dec, "pushing frame with %zu bytes at virtual address %p into VPU (system frame number: %" G_GUINT32_FORMAT ")", encoded_frame.data_size, (gpointer)(encoded_frame.data), gst_frame->system_frame_number);

	dec_ret = imx_vpu_api_dec_push_encoded_frame(imx_vpu_dec->decoder, &encoded_frame);

	gst_buffer_unmap(gst_frame->input_buffer, &in_map_info);

	if (dec_ret != IMX_VPU_API_DEC_RETURN_CODE_OK)
	{
		GST_ERROR_OBJECT(imx_vpu_dec, "could not push input frame data into decoder: %s", imx_vpu_api_dec_return_code_string(dec_ret));
		retval = FALSE;
		gst_video_codec_frame_unref(gst_frame);
		goto finish;
	}

	/* The GstVideoCodecFrame passed to handle_frame() gets ref'd prior
	 * to that call. Since we don't pass it directly to finish_frame(),
	 * drop_frame(), or release_frame() here (because we aren't done with
	 * it yet), we have to unref it here. We'll pull the frame from the
	 * GstVideoDecoder queue based on its system frame number later,
	 * and then we finish it. (gst_video_decoder_get_frame() is later
	 * used for retrieving the frame, and that function refs the frame.) */
	gst_video_codec_frame_unref(gst_frame);

finish:
	/* Re-lock the mutex since this function was called with the lock held. */
	DECODING_THREAD_LOCK(imx_vpu_dec);
	return retval;
}


static GstFlowReturn gst_imx_vpu_dec_decode_queued_frames(GstImxVpuDec *imx_vpu_dec)
{
	GstVideoDecoder *decoder = GST_VIDEO_DECODER_CAST(imx_vpu_dec);
	GstFlowReturn flow_ret = GST_FLOW_OK;
	gboolean do_loop = TRUE;
	ImxVpuApiDecReturnCodes dec_ret;
	ImxVpuApiDecOutputCodes output_code;

	do_loop = TRUE;

	do
	{
		GST_TRACE_OBJECT(imx_vpu_dec, "decoding");


		/* Main decoding block. Lock the decoder context mutex to prevent
		 * race conditions between this decoding block and something else
		 * that tries to access the context at the same time. */

		if (imx_vpu_dec->decoder_context != NULL)
			GST_IMX_VPU_DEC_CONTEXT_LOCK(imx_vpu_dec->decoder_context);

		/* If decoded frames are NOT stored in one of the framebuffers from
		 * the VPU's pool, then we have to supply a buffer manually. If
		 * there currently is none, create it and set it up. */
		if (!(imx_vpu_dec->dec_global_info->flags & IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_DECODED_FRAMES_ARE_FROM_BUFFER_POOL)
		  && (imx_vpu_dec->prepared_output_buffer == NULL)
		  && (imx_vpu_dec->dma_buffer_pool != NULL)
		)
		{
			ImxDmaBuffer *dma_buffer;
			GstBufferPoolAcquireParams params = {
				.format = GST_FORMAT_DEFAULT,
				.start = 0,
				.stop = 0,
				.flags = 0
			};

			/* Note how we do NOT acquire with the GST_IMX_VPU_DEC_BUFFER_POOL_ACQUIRE_FLAG_SELECTED
			 * flag set. This is because we want to acquire the output buffer
			 * in a regular fashion, that is, we do not want it to be a
			 * "reserved" buffer (see the GstImxVpuDecBufferPool documentation
			 * for more about this). */
			flow_ret = gst_buffer_pool_acquire_buffer(GST_BUFFER_POOL_CAST(imx_vpu_dec->dma_buffer_pool), &(imx_vpu_dec->prepared_output_buffer), &params);

			if (G_LIKELY(flow_ret == GST_FLOW_OK))
			{
				dma_buffer = gst_imx_get_dma_buffer_from_buffer(imx_vpu_dec->prepared_output_buffer);
				if (G_UNLIKELY(dma_buffer == NULL))
				{
					gst_buffer_unref(imx_vpu_dec->prepared_output_buffer);
					imx_vpu_dec->prepared_output_buffer = NULL;
					GST_ERROR_OBJECT(imx_vpu_dec, "got gstbuffer from acquire_buffer(), but it does not contain a DMA buffer");
					flow_ret = GST_FLOW_ERROR;
				}
				else
				{
					GST_LOG_OBJECT(imx_vpu_dec, "acquired gstbuffer %p with DMA buffer %p", (gpointer)(imx_vpu_dec->prepared_output_buffer), (gpointer)dma_buffer);
					/* Set this DMA buffer as the output buffer, and the GstBuffer
					 * that houses that DMA buffer as the fb context so we can
					 * retrieve it later when a frame got decoded. That way, we
					 * know what GstBuffer the frame was decoded into. */
					imx_vpu_api_dec_set_output_frame_dma_buffer(imx_vpu_dec->decoder, dma_buffer, imx_vpu_dec->prepared_output_buffer);
				}
			}

			if (G_UNLIKELY(flow_ret != GST_FLOW_OK))
			{
				GST_ERROR_OBJECT(imx_vpu_dec, "could not prepare output buffer: %s", gst_flow_get_name(flow_ret));
				goto finish;
			}
		}

		dec_ret = imx_vpu_api_dec_decode(imx_vpu_dec->decoder, &output_code);

		if (imx_vpu_dec->decoder_context != NULL)
			GST_IMX_VPU_DEC_CONTEXT_UNLOCK(imx_vpu_dec->decoder_context);


		/* Now we evaluate the outcome of our decoding attempt. */

		if (G_UNLIKELY(dec_ret != IMX_VPU_API_DEC_RETURN_CODE_OK))
		{
			GST_ERROR_OBJECT(imx_vpu_dec, "decoding frames failed: %s", imx_vpu_api_dec_return_code_string(dec_ret));
			flow_ret = GST_FLOW_ERROR;
			goto finish;
		}

		/* No error occurred, so we can now evaluate the output code. */
		switch (output_code)
		{
			case IMX_VPU_API_DEC_OUTPUT_CODE_EOS:
				GST_DEBUG_OBJECT(imx_vpu_dec, "VPU reports EOS; no more frames to decode");
				flow_ret = GST_FLOW_EOS;
				do_loop = FALSE;
				break;

			case IMX_VPU_API_DEC_OUTPUT_CODE_FRAME_SKIPPED:
			{
				guint32 system_frame_number;
				GstVideoCodecFrame *skipped_frame;
				ImxVpuApiDecSkippedFrameReasons reason;
				void *skipped_frame_context;

				imx_vpu_api_dec_get_skipped_frame_info(imx_vpu_dec->decoder, &reason, &skipped_frame_context, NULL, NULL);

				system_frame_number = (guint32)((guintptr)skipped_frame_context);
				skipped_frame = gst_video_decoder_get_frame(decoder, system_frame_number);

				if (G_UNLIKELY(skipped_frame == NULL))
				{
					GST_WARNING_OBJECT(imx_vpu_dec, "no gstframe exists with number #%" G_GUINT32_FORMAT " - cannot handle skipped frame", system_frame_number);
					goto finish;
				}

				GST_LOG_OBJECT(imx_vpu_dec, "gst frame with number #%" G_GUINT32_FORMAT " was skipped by the decoder, reason: %s (%d)", system_frame_number, imx_vpu_api_dec_skipped_frame_reason_string(reason), reason);

				skipped_frame->output_buffer = NULL;

				switch (reason)
				{
					case IMX_VPU_API_DEC_SKIPPED_FRAME_REASON_INTERNAL_FRAME:
						GST_VIDEO_CODEC_FRAME_SET_DECODE_ONLY(skipped_frame);
						flow_ret = gst_video_decoder_finish_frame(decoder, skipped_frame);
						break;

					case IMX_VPU_API_DEC_SKIPPED_FRAME_REASON_CORRUPTED_FRAME:
					default:
						flow_ret = gst_video_decoder_drop_frame(decoder, skipped_frame);
						break;
				}

				break;
			}

			case IMX_VPU_API_DEC_OUTPUT_CODE_NEW_STREAM_INFO_AVAILABLE:
			{
				GstVideoFormat video_format;
				ImxVpuApiDecStreamInfo const *new_stream_info;
				gboolean is_semi_planar;
				gboolean is_interlaced;
				gboolean is_10bit;


				/* Retrieve the new stream info. */
				new_stream_info = imx_vpu_api_dec_get_stream_info(imx_vpu_dec->decoder);
				if (G_UNLIKELY(new_stream_info == NULL))
				{
					GST_ERROR_OBJECT(imx_vpu_dec, "could not get stream info for adding framebuffers to VPU");
					flow_ret = GST_FLOW_ERROR;
					goto finish;
				}

				imx_vpu_dec->current_stream_info = *new_stream_info;


				/* Process the new stream info. */

				is_semi_planar = !!(new_stream_info->flags & IMX_VPU_API_DEC_STREAM_INFO_FLAG_SEMI_PLANAR_FRAMES);
				is_interlaced = !!(new_stream_info->flags & IMX_VPU_API_DEC_STREAM_INFO_FLAG_INTERLACED);
				is_10bit = !!(new_stream_info->flags & IMX_VPU_API_DEC_STREAM_INFO_FLAG_10BIT);

				GST_DEBUG_OBJECT(
					imx_vpu_dec,
					"framebuffer metrics:  aligned size: %zux%zu pixel  actual size: %zux%zu pixel  stride: %zu",
					new_stream_info->decoded_frame_framebuffer_metrics.aligned_frame_width,
					new_stream_info->decoded_frame_framebuffer_metrics.aligned_frame_height,
					new_stream_info->decoded_frame_framebuffer_metrics.actual_frame_width,
					new_stream_info->decoded_frame_framebuffer_metrics.actual_frame_height,
					new_stream_info->decoded_frame_framebuffer_metrics.y_stride
				);
				GST_DEBUG_OBJECT(
					imx_vpu_dec,
					"allocation info:  fb pool framebuffers: %zu bytes  output framebuffers: %zu",
					new_stream_info->min_fb_pool_framebuffer_size,
					new_stream_info->min_output_framebuffer_size
				);
				GST_DEBUG_OBJECT(
					imx_vpu_dec,
					"additional stream info:  frame rate: %u/%u  min num required framebuffers: %zu  color format: %s  semi planar: %d  interlaced: %d  10 bit depth: %d",
					new_stream_info->frame_rate_numerator, new_stream_info->frame_rate_denominator,
					new_stream_info->min_num_required_framebuffers,
					imx_vpu_api_color_format_string(new_stream_info->color_format),
					is_semi_planar,
					is_interlaced,
					is_10bit
				);

				video_format = GST_VIDEO_FORMAT_UNKNOWN;

				switch (new_stream_info->color_format)
				{
					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV420_8BIT:
						video_format = GST_VIDEO_FORMAT_I420;
						break;

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV420_10BIT:
						video_format = GST_VIDEO_FORMAT_I420_10LE;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV420_8BIT:
						video_format = GST_VIDEO_FORMAT_NV12;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV420_10BIT:
#ifdef GST_IMX_VPU_SUPPORTS_SEMI_PLANAR_10BIT_FRAMES
						video_format = GST_VIDEO_FORMAT_NV12_10LE40;
						break;
#else
						GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("GStreamer versions prior to 1.16 do not support the NV12_10LE40 format; 10-bit semi planar YUV 4:2:0 frames cannot be processed"), (NULL));
						flow_ret = GST_FLOW_ERROR;
						break;
#endif

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV411_8BIT:
						video_format = GST_VIDEO_FORMAT_Y41B;
						break;

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV411_10BIT:
						GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("GStreamer does not support 10bit YUV 4:1:1 data"), (NULL));
						flow_ret = GST_FLOW_ERROR;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV411_8BIT:
					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV411_10BIT:
						GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("GStreamer does not support semi planar YUV 4:1:1 data"), (NULL));
						flow_ret = GST_FLOW_ERROR;
						break;

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV422_HORIZONTAL_8BIT:
						video_format = GST_VIDEO_FORMAT_Y42B;
						break;

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV422_HORIZONTAL_10BIT:
						video_format = GST_VIDEO_FORMAT_I422_10LE;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV422_HORIZONTAL_8BIT:
						video_format = GST_VIDEO_FORMAT_NV16;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV422_HORIZONTAL_10BIT:
						/* GST_VIDEO_FORMAT_NV16_10LE32 is not the right format;
						 * we'd need something like GST_VIDEO_FORMAT_NV16_10LE40
						 * instead, which (currently) does not exist. */
						GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("GStreamer does not support semi planar 10bit YUV 4:2:2 horizontal data"), (NULL));
						flow_ret = GST_FLOW_ERROR;
						break;

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV422_VERTICAL_8BIT:
					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV422_VERTICAL_10BIT:
					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV422_VERTICAL_8BIT:
					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV422_VERTICAL_10BIT:
						GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("GStreamer does not support YUV 4:2:2 vertical data"), (NULL));
						flow_ret = GST_FLOW_ERROR;
						break;

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV444_8BIT:
						video_format = GST_VIDEO_FORMAT_Y444;
						break;

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV444_10BIT:
						video_format = GST_VIDEO_FORMAT_Y444_10LE;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV444_8BIT:
						video_format = GST_VIDEO_FORMAT_NV24;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV444_10BIT:
						/* There is no format like GST_VIDEO_FORMAT_NV24_10LE40. */
						GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("GStreamer does not support semi planar 10bit YUV 4:4:4 data"), (NULL));
						flow_ret = GST_FLOW_ERROR;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_P010_10BIT:
						video_format = GST_VIDEO_FORMAT_P010_10LE;
						break;

					case IMX_VPU_API_COLOR_FORMAT_YUV400_8BIT:
						video_format = GST_VIDEO_FORMAT_GRAY8;
						break;

					case IMX_VPU_API_COLOR_FORMAT_YUV400_10BIT:
						/* GST_VIDEO_FORMAT_GRAY10_LE32 is probably not the right format;
						 * but since grayscale 10-bit data is unlikely to ever occur, it
						 * is not possible to be sure. */
						GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("GStreamer does not support 10-bit grayscale data"), (NULL));
						flow_ret = GST_FLOW_ERROR;
						break;

					default:
						break;
				}

				if (video_format == GST_VIDEO_FORMAT_UNKNOWN)
				{
					GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("unsupported color format"), ("color format: %s (%d)", imx_vpu_api_color_format_string(new_stream_info->color_format), new_stream_info->color_format));
					flow_ret = GST_FLOW_ERROR;
				}

				if (flow_ret == GST_FLOW_ERROR)
					goto finish;


				/* Cleanup the old output state (if one exists) since it is
				 * likely to not be a match for our new stream info. Then set
				 * the new output state.
				 *
				 * Do this *before* calling gst_video_decoder_negotiate().
				 * This is because a valid output state is required
				 * for the negotiating process to work. */
				if (imx_vpu_dec->output_state != NULL)
				{
					gst_video_codec_state_unref(imx_vpu_dec->output_state);
					imx_vpu_dec->output_state = NULL;
				}

				/* Set the output state here, *before* negotiating.
				 * This is because a valid output state is required
				 * for the negotiating process to work. */
				imx_vpu_dec->output_state = gst_video_decoder_set_output_state(
					decoder,
					video_format,
					new_stream_info->decoded_frame_framebuffer_metrics.actual_frame_width,
					new_stream_info->decoded_frame_framebuffer_metrics.actual_frame_height,
					imx_vpu_dec->input_state
				);
				g_assert(imx_vpu_dec->output_state != NULL);


				/* Need to make sure the output state has the right interlace
				 * mode, which might not be set correctly in the input state. */

				GST_VIDEO_INFO_INTERLACE_MODE(&(imx_vpu_dec->output_state->info)) = is_interlaced ? GST_VIDEO_INTERLACE_MODE_MIXED : GST_VIDEO_INTERLACE_MODE_PROGRESSIVE;

				/* We (currently) do not support multiview. */

				GST_VIDEO_INFO_MULTIVIEW_MODE(&(imx_vpu_dec->output_state->info)) = GST_VIDEO_MULTIVIEW_MODE_MONO;
				GST_VIDEO_INFO_MULTIVIEW_FLAGS(&(imx_vpu_dec->output_state->info)) = GST_VIDEO_MULTIVIEW_FLAGS_NONE;

				/* Fill colorimetry information into the output state. */

				imx_vpu_dec->output_state->info.colorimetry.range = new_stream_info->video_full_range_flag ? GST_VIDEO_COLOR_RANGE_0_255 : GST_VIDEO_COLOR_RANGE_16_235;

				if (new_stream_info->flags & IMX_VPU_API_DEC_STREAM_INFO_FLAG_COLOR_DESCRIPTION_AVAILABLE)
				{
					/* The integer values originate from ITU-T rec. H.265 Table E.3 */
					switch (new_stream_info->color_description.color_primaries)
					{
						case 1: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_BT709; break;
						case 4: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_BT470M; break;
						case 5: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_BT470BG; break;
						case 6: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_SMPTE170M; break;
						case 7: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_SMPTE240M; break;
						case 8: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_FILM; break;
						case 9: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_BT2020; break;
						default: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_UNKNOWN; break;
					}

					/* The integer values originate from ITU-T rec. H.265 Table E.4 */
					switch (new_stream_info->color_description.transfer_characteristics)
					{
						case 1: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_BT709; break;
						case 4: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_GAMMA22; break;
						case 5: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_GAMMA28; break;
						case 7: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_SMPTE240M; break;
						case 9: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_LOG100; break;
						case 10: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_LOG316; break;
						case 13: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_SRGB; break;
						default: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_UNKNOWN; break;
					}

					/* The integer values originate from ITU-T rec. H.265 Table E.5 */
					switch (new_stream_info->color_description.matrix_coefficients)
					{
						case 0: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_RGB; break;
						case 1: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_BT709; break;
						case 4: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_FCC; break;
						case 6: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_BT601; break;
						case 7: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_SMPTE240M; break;
						case 9: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_BT2020; break;
						case 10: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_BT2020; break;
						default: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_UNKNOWN; break;
					}
				}


				/* Clean up any output buffer and DMA buffer pool that may
				 * already exist, since these are likely to not be a match for
				 * our new stream info (maybe different width/height etc.). */

				if (imx_vpu_dec->prepared_output_buffer != NULL)
				{
					gst_buffer_unref(imx_vpu_dec->prepared_output_buffer);
					imx_vpu_dec->prepared_output_buffer = NULL;
				}

				/* Unref the dma_buffer_pool and set the pointer to NULL
				 * so that decide_allocation can create a new one. */
				if (imx_vpu_dec->dma_buffer_pool != NULL)
				{
					gst_object_unref(GST_OBJECT(imx_vpu_dec->dma_buffer_pool));
					imx_vpu_dec->dma_buffer_pool = NULL;
				}


				/* This is necessary to make sure decide_allocation
				 * is called, because this creates the dma_buffer_pool. */
				gst_video_decoder_negotiate(decoder);


				/* Output state and buffer pool are both set up
				 * at this point. Create and add the framebuffers now. */
				if (new_stream_info->min_num_required_framebuffers > 0)
				{
					if (!gst_imx_vpu_dec_allocate_and_add_framebuffers(imx_vpu_dec, new_stream_info->min_num_required_framebuffers))
					{
						flow_ret = GST_FLOW_ERROR;
						goto finish;
					}
				}

				break;
			}

			case IMX_VPU_API_DEC_OUTPUT_CODE_NEED_ADDITIONAL_FRAMEBUFFER:
			{
				GST_DEBUG_OBJECT(imx_vpu_dec, "additional framebuffer requested by decoder");

				if (!gst_imx_vpu_dec_allocate_and_add_framebuffers(imx_vpu_dec, 1))
				{
					flow_ret = GST_FLOW_ERROR;
					goto finish;
				}

				break;
			}

			case IMX_VPU_API_DEC_OUTPUT_CODE_DECODED_FRAME_AVAILABLE:
			{
				ImxVpuApiRawFrame decoded_frame;

				GST_IMX_VPU_DEC_CONTEXT_LOCK(imx_vpu_dec->decoder_context);

				dec_ret = imx_vpu_api_dec_get_decoded_frame(imx_vpu_dec->decoder, &decoded_frame);

				if (dec_ret == IMX_VPU_API_DEC_RETURN_CODE_OK)
				{
					guint32 system_frame_number;
					GstVideoCodecFrame *out_frame;
					GstVideoMeta *vmeta;
					GstBuffer *gstbuffer;


					/* Get the GstBuffer that holds the framebuffer the frame was decoded into.
					 * This can be either a framebuffer from the VPU's pool, or the output buffer
					 * we set earlier with imx_vpu_api_dec_set_output_frame_dma_buffer(), depending
					 * on the IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_DECODED_FRAMES_ARE_FROM_BUFFER_POOL
					 * flag being present or not. */
					gstbuffer = GST_BUFFER_CAST(decoded_frame.fb_context);
					g_assert(gstbuffer != NULL);

					/* And get the GstVideoCodecFrame that corresponds to the frame we just
					 * decoded. Since we identify each frame with a frame number, this works
					 * even with frame reordering. Because, when frames are reordered, what
					 * happens is that the numbers we get also got reordered (because they are
					 * the context of the frames we passed into the decoder). That way, we
					 * can fetch the correct GstVideoCodecFrame instances even with reodering. */
					system_frame_number = (guint32)((guintptr)(decoded_frame.context));
					out_frame = gst_video_decoder_get_frame(decoder, system_frame_number);

					if (G_UNLIKELY(out_frame == NULL))
					{
						GST_WARNING_OBJECT(imx_vpu_dec, "no gstframe exists with number #%" G_GUINT32_FORMAT " - discarding decoded frame", system_frame_number);

						imx_vpu_api_dec_return_framebuffer_to_decoder(imx_vpu_dec->decoder, decoded_frame.fb_dma_buffer);
						GST_IMX_VPU_DEC_CONTEXT_UNLOCK(imx_vpu_dec->decoder_context);

						break;
					}

					GST_LOG_OBJECT(imx_vpu_dec, "placing decoded frame into gst frame with number #%" G_GUINT32_FORMAT, system_frame_number);

					/* Set the GstVideoCodecFrame's output_buffer. Depending on the flag
					 * IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_DECODED_FRAMES_ARE_FROM_BUFFER_POOL
					 * being present or not, this is a GstBuffer that holds one of the VPU
					 * pool framebuffers, or a GstBuffer that holds the previously set
					 * output buffer (see above). In the former case, that GstBuffer will be
					 * a "reserved" buffer (see GstImxVpuDecBufferPool for more about this). */
					if (imx_vpu_dec->dec_global_info->flags & IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_DECODED_FRAMES_ARE_FROM_BUFFER_POOL)
					{
						GstBufferPoolAcquireParams params = {
							.format = GST_FORMAT_DEFAULT,
							.start = 0,
							.stop = 0,
							.flags = (GstBufferPoolAcquireFlags)GST_IMX_VPU_DEC_BUFFER_POOL_ACQUIRE_FLAG_SELECTED
						};

						/* Select the reserved GstBuffer to make sure that the call to
						 * gst_buffer_pool_acquire_buffer() below acquires the correct one.
						 * We have to acquire the buffer even though we already have it,
						 * otherwise the GstImxVpuDecBufferPool release() function won't
						 * be called once all references to the GstBuffer are gone. This
						 * release() function _must_ be called because inside, it returns
						 * the framebuffer to the VPU's pool (which is essential, otherwise
						 * decoding will eventually fail because the VPU will run out of
						 * available framebuffers from its pool). */
						gst_imx_vpu_dec_buffer_pool_select_reserved_buffer(imx_vpu_dec->dma_buffer_pool, gstbuffer);

						flow_ret = gst_buffer_pool_acquire_buffer(GST_BUFFER_POOL_CAST(imx_vpu_dec->dma_buffer_pool), &(out_frame->output_buffer), &params);
					}
					else
					{
						out_frame->output_buffer = imx_vpu_dec->prepared_output_buffer;
						imx_vpu_dec->prepared_output_buffer = NULL;
					}

					GST_IMX_VPU_DEC_CONTEXT_UNLOCK(imx_vpu_dec->decoder_context);


					if (G_UNLIKELY(flow_ret != GST_FLOW_OK))
					{
						GST_ERROR_OBJECT(imx_vpu_dec, "could not allocate output frame: %s", gst_flow_get_name(flow_ret));
						gst_video_codec_frame_unref(out_frame);
						goto finish;
					}


					/* Set the size of the actual buffer content. Depending on which SoC  is
					 * being used, the buffers may have some reserved space at the end which
					 * is not used for video pixels. That space is used by frames that belong
					 * to the VPU's framebuffer pool; output buffers like this one don't
					 * actually ever use that extra space. min_output_framebuffer_size is the
					 * size of a buffer minus that extra space (but still with the space that
					 * is required for any padding bytes). Set the output buffer's size to
					 * this one to not confuse downstream into thinking that that extra space
					 * is actually part of the usable buffer data. */
					gst_buffer_set_size(out_frame->output_buffer, imx_vpu_dec->current_stream_info.min_output_framebuffer_size);


					/* Make a tightly packed copy of the VPU output frame buffer if needed.
					 * This function will replace the out_frame->output_buffer with the
					 * copy and unref the original out_frame->output_buffer if such a copy
					 * is required (= if imx_vpu-dec->need_to_copy_output_frames is TRUE). */
					flow_ret = gst_imx_vpu_dec_copy_output_frame_if_needed(imx_vpu_dec, out_frame);
					if (G_UNLIKELY(flow_ret != GST_FLOW_OK))
					{
						gst_video_codec_frame_unref(out_frame);
						goto finish;
					}


					/* Set the interlacing flags in the videometa if necessary. */
					if (G_LIKELY((out_frame->output_buffer != NULL) && ((vmeta = gst_buffer_get_video_meta(out_frame->output_buffer)) != NULL)))
					{
						if (imx_vpu_dec->current_stream_info.flags & IMX_VPU_API_DEC_STREAM_INFO_FLAG_INTERLACED)
						{
							GST_BUFFER_FLAG_SET(out_frame->output_buffer, GST_VIDEO_BUFFER_FLAG_INTERLACED);
							vmeta->flags |= GST_VIDEO_FRAME_FLAG_INTERLACED;
						}
						else
						{
							GST_BUFFER_FLAG_UNSET(out_frame->output_buffer, GST_VIDEO_BUFFER_FLAG_INTERLACED);
							vmeta->flags &= ~GST_VIDEO_FRAME_FLAG_INTERLACED;
						}

						switch (decoded_frame.interlacing_mode)
						{
							case IMX_VPU_API_INTERLACING_MODE_TOP_FIELD_FIRST:
								GST_BUFFER_FLAG_SET(out_frame->output_buffer, GST_VIDEO_BUFFER_FLAG_TFF);
								GST_BUFFER_FLAG_UNSET(out_frame->output_buffer, GST_VIDEO_BUFFER_FLAG_ONEFIELD);
								vmeta->flags |= GST_VIDEO_FRAME_FLAG_TFF;
								vmeta->flags &= ~GST_VIDEO_FRAME_FLAG_ONEFIELD;
								break;

							case IMX_VPU_API_INTERLACING_MODE_TOP_FIELD_ONLY:
								GST_BUFFER_FLAG_SET(out_frame->output_buffer, GST_VIDEO_BUFFER_FLAG_TFF);
								GST_BUFFER_FLAG_SET(out_frame->output_buffer, GST_VIDEO_BUFFER_FLAG_ONEFIELD);
								vmeta->flags |= GST_VIDEO_FRAME_FLAG_TFF;
								vmeta->flags |= GST_VIDEO_FRAME_FLAG_ONEFIELD;
								break;

							default:
								break;
						}
					}


					/* We have finished processing the decoded frame. */
					flow_ret = gst_video_decoder_finish_frame(decoder, out_frame);
				}
				else
				{
					GST_IMX_VPU_DEC_CONTEXT_UNLOCK(imx_vpu_dec->decoder_context);
					GST_ERROR_OBJECT(imx_vpu_dec, "could not retrieve decoded frame: %s", imx_vpu_api_dec_return_code_string(dec_ret));
					goto finish;
				}

				break;
			}

			case IMX_VPU_API_DEC_OUTPUT_CODE_MORE_INPUT_DATA_NEEDED:
				GST_LOG_OBJECT(imx_vpu_dec, "VPU has no more data to decode");
				do_loop = FALSE;
				break;

			default:
				break;
		}
	}
	while (do_loop);

finish:
	return flow_ret;
}


static void gst_imx_vpu_dec_unref_decoder_context(GstImxVpuDec *imx_vpu_dec)
{
	if (imx_vpu_dec->decoder_context == NULL)
		return;

	/* Close the decoder right now to make sure it is closed by the
	 * time this call ends, even if there are more references to the
	 * decoder context somewhere. Otherwise, the imxvpuapi decoder
	 * would be closed only once all of these refs are unref'd and
	 * the decoder context finalizer kicks in. */
	gst_imx_vpu_dec_context_close_decoder(imx_vpu_dec->decoder_context);

	gst_object_unref(GST_OBJECT(imx_vpu_dec->decoder_context));
	imx_vpu_dec->decoder_context = NULL;
}


static void gst_imx_vpu_dec_teardown_current_decoder(GstImxVpuDec *imx_vpu_dec)
{
	/* Cleanup old decoder context. */
	gst_imx_vpu_dec_unref_decoder_context(imx_vpu_dec);

	/* Clean up the old codec data copy. */
	if (imx_vpu_dec->codec_data != NULL)
	{
		GST_DEBUG_OBJECT(imx_vpu_dec, "cleaning up existing codec data gstbuffer %p", (gpointer)(imx_vpu_dec->codec_data));

		if (imx_vpu_dec->codec_data_is_mapped)
		{
			gst_buffer_unmap(imx_vpu_dec->codec_data, &(imx_vpu_dec->codecdata_map_info));
			imx_vpu_dec->codec_data_is_mapped = FALSE;
		}

		GST_DEBUG_OBJECT(imx_vpu_dec, "unref'ing codec data gstbuffer %p", (gpointer)(imx_vpu_dec->codec_data));
		gst_buffer_unref(imx_vpu_dec->codec_data);
		imx_vpu_dec->codec_data = NULL;
	}

	if (imx_vpu_dec->prepared_output_buffer != NULL)
	{
		gst_buffer_unref(imx_vpu_dec->prepared_output_buffer);
		imx_vpu_dec->prepared_output_buffer = NULL;
	}

	if (imx_vpu_dec->dma_buffer_pool != NULL)
	{
		gst_object_unref(GST_OBJECT(imx_vpu_dec->dma_buffer_pool));
		imx_vpu_dec->dma_buffer_pool = NULL;
	}

	if (imx_vpu_dec->nonvideometa_output_buffer_pool != NULL)
	{
		gst_object_unref(GST_OBJECT(imx_vpu_dec->nonvideometa_output_buffer_pool));
		imx_vpu_dec->nonvideometa_output_buffer_pool = NULL;
	}

	/* Clean up old input and output states. */
	if (imx_vpu_dec->input_state != NULL)
	{
		gst_video_codec_state_unref(imx_vpu_dec->input_state);
		imx_vpu_dec->input_state = NULL;
	}
	if (imx_vpu_dec->output_state != NULL)
	{
		gst_video_codec_state_unref(imx_vpu_dec->output_state);
		imx_vpu_dec->output_state = NULL;
	}
}


static gboolean gst_imx_vpu_dec_allocate_and_add_framebuffers(GstImxVpuDec *imx_vpu_dec, size_t num_framebuffers)
{
	size_t i;
	gboolean ret = TRUE;
	ImxVpuApiDecReturnCodes dec_ret;
	ImxDmaBuffer **dma_buffers = NULL;
	void **fb_contexts = NULL;

	g_assert(imx_vpu_dec->dma_buffer_pool != NULL);
	g_assert(num_framebuffers > 0);

	dma_buffers = g_malloc0(sizeof(ImxDmaBuffer *) * num_framebuffers);
	fb_contexts = g_malloc0(sizeof(void *) * num_framebuffers);

	GST_IMX_VPU_DEC_CONTEXT_LOCK(imx_vpu_dec->decoder_context);

	for (i = 0; i < num_framebuffers; ++i)
	{
		GstBuffer *reserved_buffer;
		ImxDmaBuffer *dma_buffer;

		reserved_buffer = gst_imx_vpu_dec_buffer_pool_reserve_buffer(imx_vpu_dec->dma_buffer_pool);
		if (G_UNLIKELY(reserved_buffer == NULL))
		{
			GST_ERROR_OBJECT(imx_vpu_dec, "could not reserve dma_buffer");
			ret = FALSE;
			goto finish;
		}

		dma_buffer = gst_imx_get_dma_buffer_from_buffer(reserved_buffer);
		if (G_UNLIKELY(dma_buffer == NULL))
		{
			gst_buffer_unref(reserved_buffer);
			GST_ERROR_OBJECT(imx_vpu_dec, "got gstbuffer from reserve_buffer(), but it does not contain a DMA buffer");
			ret = FALSE;
			goto finish;
		}
		dma_buffers[i] = dma_buffer;
		fb_contexts[i] = reserved_buffer;
	}

	if ((dec_ret = imx_vpu_api_dec_add_framebuffers_to_pool(imx_vpu_dec->decoder, dma_buffers, fb_contexts, num_framebuffers)) != IMX_VPU_API_DEC_RETURN_CODE_OK)
	{
		GST_ERROR_OBJECT(imx_vpu_dec, "could not add framebuffers to decoder pool: %s", imx_vpu_api_dec_return_code_string(dec_ret));
		ret = FALSE;
		goto finish;
	}


finish:
	/* The buffers that were allocated and reserved earlier by calling the
	 * gst_imx_vpu_dec_buffer_pool_reserve_buffer() function are NOT
	 * unref'd here. That is because they were allocated but not
	 * acquired. If the latter step is not done, they are not pooled
	 * properly, and therefore are not released into the buffer pool
	 * when they are unref'd.
	 *
	 * It turns out that it is not necessary to deallocate/unref these
	 * reserved buffers. That is because an error while adding framebuffers
	 * is fatal, will cause FALSE to be returned, and the callers will
	 * stop any attempts at decoding. So, the pipeline will be shut down,
	 * the buffer pool will be shut down as well, which in turn will cause
	 * it to internally unref any reserved buffers. In short, no resource
	 * leaks will occur. */

	GST_IMX_VPU_DEC_CONTEXT_UNLOCK(imx_vpu_dec->decoder_context);
	g_free(dma_buffers);
	g_free(fb_contexts);
	return ret;

	goto finish;
}


static GstFlowReturn gst_imx_vpu_dec_copy_output_frame_if_needed(GstImxVpuDec *imx_vpu_dec, GstVideoCodecFrame *output_frame)
{
	GstFlowReturn flow_ret;
	GstVideoFrame vpu_output_video_frame;
	GstVideoFrame new_output_video_frame;
	GstBuffer *new_output_buffer;
	GstVideoInfo vpu_video_info;

	if (!(imx_vpu_dec->need_to_copy_output_frames))
		return GST_FLOW_OK;

	flow_ret = gst_buffer_pool_acquire_buffer(imx_vpu_dec->nonvideometa_output_buffer_pool, &new_output_buffer, NULL);
	if (G_UNLIKELY(flow_ret != GST_FLOW_OK))
	{
		GST_ERROR_OBJECT(imx_vpu_dec, "could not allocate output buffer with nonvideometa buffer pool: %s", gst_flow_get_name(flow_ret));
		goto error;
	}

	memcpy(&vpu_video_info, gst_imx_vpu_dec_buffer_pool_get_video_info(imx_vpu_dec->dma_buffer_pool), sizeof(GstVideoInfo));

	if (!gst_video_frame_map(
		&vpu_output_video_frame,
		&vpu_video_info,
		output_frame->output_buffer,
		GST_MAP_READ
	))
	{
		GST_ERROR_OBJECT(imx_vpu_dec, "could not map VPU output video frame");
		goto error;
	}

	if (!gst_video_frame_map(
		&new_output_video_frame,
		&(imx_vpu_dec->nonvideometa_output_video_info),
		new_output_buffer,
		GST_MAP_WRITE
	))
	{
		GST_ERROR_OBJECT(imx_vpu_dec, "could not map new output video frame");
		goto error;
	}

	if (!gst_video_frame_copy(&new_output_video_frame, &vpu_output_video_frame))
	{
		GST_ERROR_OBJECT(imx_vpu_dec, "could not copy pixels from VPU output buffer into new output buffer");
		goto error;
	}

	GST_LOG_OBJECT(
		imx_vpu_dec,
		"copied pixels from VPU output buffer into new output buffer"
	);

	gst_buffer_unref(output_frame->output_buffer);
	output_frame->output_buffer = new_output_buffer;

finish:
	gst_video_frame_unmap(&new_output_video_frame);
	gst_video_frame_unmap(&vpu_output_video_frame);

	return flow_ret;

error:
	gst_buffer_unref(new_output_buffer);
	goto finish;
}




/* class_init function for autogenerated subclasses. */
static void derived_class_init(void *klass)
{
	GstImxVpuDecClass *imx_vpu_dec_class;
	GstElementClass *element_class;
	GstPadTemplate *sink_template;
	GstPadTemplate *src_template;
	GstCaps *sink_template_caps;
	GstCaps *src_template_caps;
	gboolean got_caps;
	gchar *longname;
	gchar *classification;
	gchar *description;
	gchar *author;
	ImxVpuApiCompressionFormat compression_format;
	GstImxVpuCodecDetails const *codec_details;

	imx_vpu_dec_class = GST_IMX_VPU_DEC_CLASS(klass);
	element_class = GST_ELEMENT_CLASS(klass);

	compression_format = (ImxVpuApiCompressionFormat)g_type_get_qdata(G_OBJECT_CLASS_TYPE(klass), gst_imx_vpu_compression_format_quark());
	codec_details = gst_imx_vpu_get_codec_details(compression_format);
	g_assert(codec_details != NULL);

	got_caps = gst_imx_vpu_get_caps_for_format(codec_details->compression_format, imx_vpu_api_dec_get_compression_format_support_details(codec_details->compression_format), &sink_template_caps, &src_template_caps, FALSE);
	g_assert(got_caps);

	sink_template = gst_pad_template_new("sink", GST_PAD_SINK, GST_PAD_ALWAYS, sink_template_caps);
	src_template = gst_pad_template_new("src", GST_PAD_SRC, GST_PAD_ALWAYS, src_template_caps);

	gst_element_class_add_pad_template(element_class, sink_template);
	gst_element_class_add_pad_template(element_class, src_template);

	imx_vpu_dec_class->is_frame_reordering_required = codec_details->is_frame_reordering_required;
	imx_vpu_dec_class->requires_codec_data = codec_details->requires_codec_data;

	longname = g_strdup_printf("i.MX VPU %s video decoder", codec_details->desc_name);
	classification = g_strdup("Codec/Decoder/Video/Hardware");
	description = g_strdup_printf("Hardware-accelerated %s video decoding using the i.MX VPU codec", codec_details->desc_name);
	author = g_strdup("Carlos Rafael Giani <crg7475@mailbox.org>");
	gst_element_class_set_metadata(element_class, longname, classification, description, author);
	g_free(longname);
	g_free(classification);
	g_free(description);
	g_free(author);
}


GTypeInfo gst_imx_vpu_dec_get_derived_type_info(void)
{
	GTypeInfo type_info =
	{
		sizeof(GstImxVpuDecClass),
		NULL,
		NULL,
		(GClassInitFunc)(void (*)(void))derived_class_init,
		NULL,
		NULL,
		sizeof(GstImxVpuDec),
		0,
		NULL,
		NULL
	};

	return type_info;
}


gboolean gst_imx_vpu_dec_register_decoder_type(GstPlugin *plugin, ImxVpuApiCompressionFormat compression_format)
{
	GType type;
	gchar *element_name, *type_name;
	gboolean ret = FALSE;
	GTypeInfo typeinfo = gst_imx_vpu_dec_get_derived_type_info();
	GstImxVpuCodecDetails const * codec_details = gst_imx_vpu_get_codec_details(compression_format);

	element_name = g_strdup_printf("imxvpudec_%s", codec_details->element_name_suffix);
	type_name = g_strdup_printf("GstImxVpuDec%s", codec_details->class_name_suffix);
	type = g_type_from_name(type_name);
	if (!type)
	{
		type = g_type_register_static(GST_TYPE_IMX_VPU_DEC, type_name, &typeinfo, 0);
		/* Set the compression format as qdata. The cast to a gpointer is safe,
		 * since we never dereference this "pointer" anyway. */
		g_type_set_qdata(type, gst_imx_vpu_compression_format_quark(), (gpointer)compression_format);
	}

	ret = gst_element_register(plugin, element_name, codec_details->rank, type);

	g_free(type_name);

	return ret;
}
