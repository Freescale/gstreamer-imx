/* gstreamer-imx: GStreamer plugins for the i.MX SoCs
 * Copyright (C) 2019  Carlos Rafael Giani
 *
 * This library is free software; you can redistribute it and/or
 * modify it under the terms of the GNU Library General Public
 * License as published by the Free Software Foundation; either
 * version 2 of the License, or (at your option) any later version.
 *
 * This library is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * Library General Public License for more details.
 *
 * You should have received a copy of the GNU Library General Public
 * License along with this library; if not, write to the Free
 * Software Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 */

#include <gst/gst.h>
#include <gst/allocators/allocators.h>
#include <gst/video/gstvideodecoder.h>
#include <gst/video/gstvideometa.h>
#include <imxdmabuffer/imxdmabuffer.h>
#include <imxdmabuffer/imxdmabuffer_config.h>
#include <imxvpuapi2/imxvpuapi2.h>
#include "gst/imx/gstimxdmabufferallocator.h"
#include "gstimxvpudec.h"
#include "gstimxvpudeccontext.h"
#include "gstimxvpudecbufferpool.h"
#include "gstimxvpucommon.h"


GST_DEBUG_CATEGORY_STATIC(imx_vpu_dec_debug);
#define GST_CAT_DEFAULT imx_vpu_dec_debug


/* This is the base class for decoder elements. Derived classes
 * are not implemented manually. Rather, they are procedurally
 * generated out of information from the GstImxVpuCodecDetails
 * table that is stored in gstimxvpucommon.c and access by calling
 * gst_imx_vpu_get_codec_details(). The autogeneration and
 * registration is done by gst_imx_vpu_dec_register_decoder_type().
 * To let the autogenerated subclass know what format it is supposed
 * to handle, the libimxvpuapi compression format enum is stored
 * as qdata in the class derived from GstImxVpuDecClass. That qdata
 * is accessed using gst_imx_vpu_compression_format_quark(). */


struct _GstImxVpuDec
{
	GstVideoDecoder parent;

	/* Out-of-band codec data along with mapping information.
	 * See the code in gst_imx_vpu_dec_set_format() for details. */
	GstBuffer *codec_data;
	GstMapInfo codecdata_map_info;
	gboolean codec_data_is_mapped;

	/* Input and output video codec states. The input state is
	 * set in gst_imx_vpu_dec_set_format(). The output state is
	 * set in gst_imx_vpu_dec_decode_queued_frames() once the
	 * IMX_VPU_API_DEC_OUTPUT_CODE_NEW_STREAM_INFO_AVAILABLE
	 * output code is received. */
	GstVideoCodecState *input_state;
	GstVideoCodecState *output_state;

	/* Copy of the stream info received when the output code
	 * IMX_VPU_API_DEC_OUTPUT_CODE_NEW_STREAM_INFO_AVAILABLE
	 * appears. */
	ImxVpuApiDecStreamInfo current_stream_info;

	/* Current decoder context. Created in
	 * gst_imx_vpu_dec_set_format(). */
	GstImxVpuDecContext *decoder_context;

	/* Current DMA buffer pool. Created in
	 * gst_imx_vpu_dec_decide_allocation(). */
	GstImxVpuDecBufferPool *dma_buffer_pool;

	/* The output buffer that is prepared for receiving
	 * a decoded output frame. This is only used if the
	 * IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_DECODED_FRAMES_ARE_FROM_BUFFER_POOL
	 * global imxvpuapi decoder flag is _not_ set. See the
	 * libimxvpuapi documentation for more details. */
	GstBuffer *prepared_output_buffer;

	/* The stream buffer that is needed by the decoder for all
	 * of its decoding operations. Created in gst_imx_vpu_dec_start(). */
	GstMemory *stream_buffer;
	/* The actual libimxvpuapi decoder. Created in
	 * gst_imx_vpu_dec_set_format(). Gets destroyed in
	 * the decoder context' finalizer. */
	ImxVpuApiDecoder *decoder;
	/* Pointer to the constant, static global decoder
	 * information from libimxvpuapi. */
	ImxVpuApiDecGlobalInfo const *dec_global_info;
	/* The parameters that are passed on to the imx_vpu_api_dec_open()
	 * call that opens a libimxvpuapi decoder instance. */
	ImxVpuApiDecOpenParams open_params;
	/* libimxdmabuffer-based DMA buffer allocator that is used for
	 * allocating the stream buffer and the VPU framebuffer pool buffers.
	 * Depending on the configuration, this may or may not be the
	 * DMA-BUF backed GstImxIonAllocator. */
	GstAllocator *default_dma_buf_allocator;

	/* Sometimes, even after one of the GstVideoDecoder vfunctions
	 * reports an error, processing continues. This flag is intended
	 * to handle such cases. If set to TRUE, several functions such as
	 * gst_imx_vpu_dec_handle_frame() will exit early. The flag is
	 * cleared once the decoder is restarted. */
	gboolean fatal_error_cannot_decode;
};


struct _GstImxVpuDecClass
{
	GstVideoDecoderClass parent_class;

	/* This is a copy of the flag from the entry from the
	 * GstImxVpuCodecDetails table that corresponds to the compression
	 * format the decoder handles. */
	gboolean requires_codec_data;
};


G_DEFINE_ABSTRACT_TYPE(GstImxVpuDec, gst_imx_vpu_dec, GST_TYPE_VIDEO_DECODER);


static gboolean gst_imx_vpu_dec_start(GstVideoDecoder *decoder);
static gboolean gst_imx_vpu_dec_stop(GstVideoDecoder *decoder);
static gboolean gst_imx_vpu_dec_set_format(GstVideoDecoder *decoder, GstVideoCodecState *state);
static GstFlowReturn gst_imx_vpu_dec_handle_frame(GstVideoDecoder *decoder, GstVideoCodecFrame *cur_frame);
static gboolean gst_imx_vpu_dec_flush(GstVideoDecoder *decoder);
static gboolean gst_imx_vpu_dec_drain(GstVideoDecoder *decoder);
static GstFlowReturn gst_imx_vpu_dec_finish(GstVideoDecoder *decoder);
static gboolean gst_imx_vpu_dec_decide_allocation(GstVideoDecoder *decoder, GstQuery *query);

static GstFlowReturn gst_imx_vpu_dec_decode_queued_frames(GstImxVpuDec *imx_vpu_dec);
static void gst_imx_vpu_dec_unref_decoder_context(GstImxVpuDec *imx_vpu_dec);
static gboolean gst_imx_vpu_dec_allocate_and_add_framebuffers(GstImxVpuDec *imx_vpu_dec, size_t num_framebuffers);


static void gst_imx_vpu_dec_class_init(GstImxVpuDecClass *klass)
{
	GstVideoDecoderClass *video_decoder_class;

	gst_imx_vpu_api_setup_logging();

	GST_DEBUG_CATEGORY_INIT(imx_vpu_dec_debug, "imxvpudec", 0, "NXP i.MX VPU video decoder");

	video_decoder_class = GST_VIDEO_DECODER_CLASS(klass);

	video_decoder_class->start             = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_start);
	video_decoder_class->stop              = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_stop);
	video_decoder_class->set_format        = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_set_format);
	video_decoder_class->handle_frame      = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_handle_frame);
	video_decoder_class->flush             = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_flush);
	video_decoder_class->drain             = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_drain);
	video_decoder_class->finish            = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_finish);
	video_decoder_class->decide_allocation = GST_DEBUG_FUNCPTR(gst_imx_vpu_dec_decide_allocation);
}


static void gst_imx_vpu_dec_init(GstImxVpuDec *imx_vpu_dec)
{
	imx_vpu_dec->codec_data = NULL;
	imx_vpu_dec->codec_data_is_mapped = FALSE;

	imx_vpu_dec->input_state = NULL;
	imx_vpu_dec->output_state = NULL;

	imx_vpu_dec->decoder_context = NULL;
	imx_vpu_dec->dma_buffer_pool = NULL;
	imx_vpu_dec->prepared_output_buffer = NULL;

	imx_vpu_dec->stream_buffer = NULL;
	imx_vpu_dec->decoder = NULL;
	imx_vpu_dec->dec_global_info = imx_vpu_api_dec_get_global_info();
	memset(&(imx_vpu_dec->open_params), 0, sizeof(imx_vpu_dec->open_params));
	imx_vpu_dec->default_dma_buf_allocator = NULL;

	imx_vpu_dec->fatal_error_cannot_decode = FALSE;
}


static gboolean gst_imx_vpu_dec_start(GstVideoDecoder *decoder)
{
	gboolean ret = TRUE;
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC(decoder);
	size_t stream_buffer_size, stream_buffer_alignment;
	GstAllocationParams alloc_params;
	ImxVpuApiCompressionFormat compression_format = GST_IMX_VPU_GET_ELEMENT_COMPRESSION_FORMAT(decoder);
	GstImxVpuCodecDetails const * codec_details = gst_imx_vpu_get_codec_details(compression_format);

	imx_vpu_dec->fatal_error_cannot_decode = FALSE;


	/* Set up the stream buffer */

	stream_buffer_size = imx_vpu_dec->dec_global_info->min_required_stream_buffer_size;
	stream_buffer_alignment = imx_vpu_dec->dec_global_info->required_stream_buffer_physaddr_alignment;

	GST_DEBUG_OBJECT(
		imx_vpu_dec,
		"stream buffer info:  required min size: %zu bytes  required alignment: %zu  decoded frames are from buffer pool: %d",
		stream_buffer_size,
		stream_buffer_alignment,
		!!(imx_vpu_dec->dec_global_info->flags & IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_DECODED_FRAMES_ARE_FROM_BUFFER_POOL)
	);

	memset(&alloc_params, 0, sizeof(alloc_params));
	alloc_params.align = stream_buffer_alignment - 1;

	imx_vpu_dec->default_dma_buf_allocator = gst_imx_allocator_new();

	imx_vpu_dec->stream_buffer = gst_allocator_alloc(
		imx_vpu_dec->default_dma_buf_allocator,
		stream_buffer_size,
		&alloc_params
	);
	if (G_UNLIKELY(imx_vpu_dec->stream_buffer == NULL))
	{
		GST_ELEMENT_ERROR(imx_vpu_dec, RESOURCE, FAILED, ("could not allocate DMA memory for stream buffer"), (NULL));
		ret = FALSE;
		goto finish;
	}


	/* VPU decoder setup continues in set_format(), since we need to
	 * know the input caps to fill the open_params structure. */

	GST_INFO_OBJECT(imx_vpu_dec, "i.MX VPU %s decoder started", codec_details->desc_name);


finish:
	return ret;
}


static gboolean gst_imx_vpu_dec_stop(GstVideoDecoder *decoder)
{
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC(decoder);
	ImxVpuApiCompressionFormat compression_format = GST_IMX_VPU_GET_ELEMENT_COMPRESSION_FORMAT(decoder);
	GstImxVpuCodecDetails const * codec_details = gst_imx_vpu_get_codec_details(compression_format);

	gst_imx_vpu_dec_unref_decoder_context(imx_vpu_dec);

	if (imx_vpu_dec->prepared_output_buffer != NULL)
	{
		gst_buffer_unref(imx_vpu_dec->prepared_output_buffer);
		imx_vpu_dec->prepared_output_buffer = NULL;
	}

	if (imx_vpu_dec->dma_buffer_pool != NULL)
	{
		gst_object_unref(GST_OBJECT(imx_vpu_dec->dma_buffer_pool));
		imx_vpu_dec->dma_buffer_pool = NULL;
	}

	if (imx_vpu_dec->stream_buffer != NULL)
	{
		gst_memory_unref(imx_vpu_dec->stream_buffer);
		imx_vpu_dec->stream_buffer = NULL;
	}

	if (imx_vpu_dec->codec_data != NULL)
	{
		if (imx_vpu_dec->codec_data_is_mapped)
		{
			gst_buffer_unmap(imx_vpu_dec->codec_data, &(imx_vpu_dec->codecdata_map_info));
			imx_vpu_dec->codec_data_is_mapped = FALSE;
		}

		GST_DEBUG_OBJECT(imx_vpu_dec, "unref'ing codec data gstbuffer %p", (gpointer)(imx_vpu_dec->codec_data));
		gst_buffer_unref(imx_vpu_dec->codec_data);
		imx_vpu_dec->codec_data = NULL;
	}

	if (imx_vpu_dec->input_state != NULL)
	{
		gst_video_codec_state_unref(imx_vpu_dec->input_state);
		imx_vpu_dec->input_state = NULL;
	}

	if (imx_vpu_dec->output_state != NULL)
	{
		gst_video_codec_state_unref(imx_vpu_dec->output_state);
		imx_vpu_dec->output_state = NULL;
	}

	if (imx_vpu_dec->default_dma_buf_allocator != NULL)
	{
		gst_object_unref(GST_OBJECT(imx_vpu_dec->default_dma_buf_allocator));
		imx_vpu_dec->default_dma_buf_allocator = NULL;
	}

	GST_INFO_OBJECT(imx_vpu_dec, "i.MX VPU %s decoder stopped", codec_details->desc_name);

	return TRUE;
}


static gboolean gst_imx_vpu_dec_set_format(GstVideoDecoder *decoder, GstVideoCodecState *state)
{
	ImxVpuApiDecReturnCodes dec_ret;
	GstVideoFormat downstream_format;
	gboolean ret = TRUE;
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC_CAST(decoder);
	GstImxVpuDecClass *klass = GST_IMX_VPU_DEC_CLASS(G_OBJECT_GET_CLASS(decoder));
	ImxVpuApiDecGlobalInfo const *dec_global_info = imx_vpu_dec->dec_global_info;
	ImxVpuApiDecOpenParams *open_params = &(imx_vpu_dec->open_params);
	GstCaps *allowed_srccaps = NULL;
	ImxVpuApiCompressionFormat compression_format = GST_IMX_VPU_GET_ELEMENT_COMPRESSION_FORMAT(decoder);

	GST_DEBUG_OBJECT(decoder, "setting decoder format");


	/* Drain frames that are already decoded but not yet displayed. */
	GST_DEBUG_OBJECT(decoder, "draining remaining frames from decoder");
	if ((imx_vpu_dec->decoder != NULL) && (gst_imx_vpu_dec_decode_queued_frames(imx_vpu_dec) != GST_FLOW_OK))
	{
		ret = FALSE;
		goto finish;
	}


	/* Cleanup any existing data and states. */

	/* Cleanup old decoder context. */
	gst_imx_vpu_dec_unref_decoder_context(imx_vpu_dec);

	/* Clean up the old codec data copy. */
	if (imx_vpu_dec->codec_data != NULL)
	{
		GST_DEBUG_OBJECT(decoder, "cleaning up existing codec data gstbuffer %p", (gpointer)(imx_vpu_dec->codec_data));

		if (imx_vpu_dec->codec_data_is_mapped)
		{
			gst_buffer_unmap(imx_vpu_dec->codec_data, &(imx_vpu_dec->codecdata_map_info));
			imx_vpu_dec->codec_data_is_mapped = FALSE;
		}

		gst_buffer_unref(imx_vpu_dec->codec_data);
		imx_vpu_dec->codec_data = NULL;
	}

	if (imx_vpu_dec->prepared_output_buffer != NULL)
	{
		gst_buffer_unref(imx_vpu_dec->prepared_output_buffer);
		imx_vpu_dec->prepared_output_buffer = NULL;
	}

	if (imx_vpu_dec->dma_buffer_pool != NULL)
	{
		gst_object_unref(GST_OBJECT(imx_vpu_dec->dma_buffer_pool));
		imx_vpu_dec->dma_buffer_pool = NULL;
	}

	/* Clean up old input and output states. */
	if (imx_vpu_dec->input_state != NULL)
	{
		gst_video_codec_state_unref(imx_vpu_dec->input_state);
		imx_vpu_dec->input_state = NULL;
	}
	if (imx_vpu_dec->output_state != NULL)
	{
		gst_video_codec_state_unref(imx_vpu_dec->output_state);
		imx_vpu_dec->output_state = NULL;
	}


	/* Get the caps that downstream allows. Amongst other things, this allows
	 * us to pick a video format that is suitable for decoding. */
	{
		gchar const *format_str;
		GValue const *format_value;
		downstream_format = GST_VIDEO_FORMAT_UNKNOWN;

		allowed_srccaps = gst_pad_get_allowed_caps(GST_VIDEO_DECODER_SRC_PAD(decoder));

		if (allowed_srccaps != NULL)
		{
			/* Look at the sample format values from the first structure */
			GstStructure *structure = gst_caps_get_structure(allowed_srccaps, 0);
			format_value = gst_structure_get_value(structure, "format");

			if (format_value == NULL)
			{
				ret = FALSE;
				goto finish;
			}
			else if (GST_VALUE_HOLDS_LIST(format_value))
			{
				/* if value is a format list, pick the first entry */
				GValue const *fmt_list_value = gst_value_list_get_value(format_value, 0);
				format_str = g_value_get_string(fmt_list_value);
			}
			else if (G_VALUE_HOLDS_STRING(format_value))
			{
				/* if value is a string, use it directly */
				format_str = g_value_get_string(format_value);
			}
			else
			{
				GST_ERROR_OBJECT(imx_vpu_dec, "unexpected type for 'format' field in allowed_srccaps structure %" GST_PTR_FORMAT, structure);
				ret = FALSE;
				goto finish;
			}

			downstream_format = gst_video_format_from_string(format_str);
			g_assert(downstream_format != GST_VIDEO_FORMAT_UNKNOWN);
		}
	}


	/* Begin filling the open_params. */

	memset(open_params, 0, sizeof(ImxVpuApiDecOpenParams));

	/* Get the codec data if required. */
	if (klass->requires_codec_data)
	{
		GValue const *value;
		GstStructure *structure;

		structure = gst_caps_get_structure(state->caps, 0);

		value = gst_structure_get_value(structure, "codec_data");
		if (value != NULL)
		{
			GstBuffer *codec_data;
			GST_DEBUG_OBJECT(imx_vpu_dec, "codec data expected and found in caps");
			codec_data = gst_value_get_buffer(value);

			/* Copy the codec data buffer, to make sure the codec_data
	 		 * lifetime does not depend on the caps. */
			imx_vpu_dec->codec_data = gst_buffer_copy(codec_data);
		}
		else
		{
			GST_ERROR_OBJECT(imx_vpu_dec, "codec data expected, but not found in caps");
			ret = FALSE;
			goto finish;
		}
	}

	/* We want the VPU to reorder the frames. We can keep track of this reordering
	 * through the GstVideoDecoder system frame numbers. */
	open_params->flags |= IMX_VPU_API_DEC_OPEN_PARAMS_FLAG_ENABLE_FRAME_REORDERING;
	open_params->frame_width = state->info.width;
	open_params->frame_height = state->info.height;
	open_params->compression_format = compression_format;


	/* Check if 10-bit decoding is required. */

	if (allowed_srccaps == NULL)
	{
		open_params->flags |= IMX_VPU_API_DEC_OPEN_PARAMS_FLAG_USE_10BIT_DECODING;
		GST_DEBUG_OBJECT(imx_vpu_dec, "srcpad not linked (yet), so no src caps set; enabling 10-bit decoding by default");
	}
	else if (gst_imx_vpu_color_format_has_10bit(downstream_format))
	{
		open_params->flags |= IMX_VPU_API_DEC_OPEN_PARAMS_FLAG_USE_10BIT_DECODING;
		GST_DEBUG_OBJECT(imx_vpu_dec, "format %s detected in list of supported srccaps formats; enabling 10-bit decoding", gst_video_format_to_string(downstream_format));
	}


	/* Check if fully planar or semi planar frames shall be produced. */

	if ((dec_global_info->flags & (IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_FULLY_PLANAR_FRAMES_SUPPORTED | IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_SEMI_PLANAR_FRAMES_SUPPORTED)) == (IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_FULLY_PLANAR_FRAMES_SUPPORTED | IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_SEMI_PLANAR_FRAMES_SUPPORTED))
	{
		/* Find out what formats downstream supports, to determine
		 * if we have to request fully planar or semi planar data. */

		if (allowed_srccaps == NULL)
		{
			open_params->flags |= IMX_VPU_API_DEC_OPEN_PARAMS_FLAG_USE_SEMI_PLANAR_COLOR_FORMAT;
			GST_DEBUG_OBJECT(imx_vpu_dec, "srcpad not linked (yet), so no src caps set; enabling semi planar data by default");
		}
		else
		{
			gboolean is_semi_planar = gst_imx_vpu_color_format_is_semi_planar(downstream_format);
			if (is_semi_planar)
				open_params->flags |= IMX_VPU_API_DEC_OPEN_PARAMS_FLAG_USE_SEMI_PLANAR_COLOR_FORMAT;

			GST_DEBUG_OBJECT(imx_vpu_dec, "format %s detected in list of supported srccaps formats; setting semi planar data flag in open params to %d", gst_video_format_to_string(downstream_format), is_semi_planar);
		}
	}
	else if (dec_global_info->flags & IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_SEMI_PLANAR_FRAMES_SUPPORTED)
	{
		GST_DEBUG_OBJECT(imx_vpu_dec, "decoder only supports semi planar formats");
		open_params->flags |= IMX_VPU_API_DEC_OPEN_PARAMS_FLAG_USE_SEMI_PLANAR_COLOR_FORMAT;
	}
	else if (dec_global_info->flags & IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_FULLY_PLANAR_FRAMES_SUPPORTED)
	{
		GST_DEBUG_OBJECT(imx_vpu_dec, "decoder only supports fully planar formats");
	}


	/* Map the codec data since the decoder needs to be
	  able to access it in the handle_frame() calls. */
	if (imx_vpu_dec->codec_data != NULL)
	{
		gst_buffer_map(imx_vpu_dec->codec_data, &(imx_vpu_dec->codecdata_map_info), GST_MAP_READ);
		imx_vpu_dec->codec_data_is_mapped = TRUE;

		/* The decoder does _not_ copy the bytes from codec_data. Instead, it
		 * just stores a pointer to the codec data, which is why we must keep
		 * codec_data mapped, and why codec_data must exist at least for the
		 * duration of the decoding. */
		GST_LOG_OBJECT(imx_vpu_dec, "using extra codec data (%" G_GSIZE_FORMAT " byte) from gstbuffer %p", imx_vpu_dec->codecdata_map_info.size, (gpointer)(imx_vpu_dec->codec_data));
		open_params->extra_header_data = imx_vpu_dec->codecdata_map_info.data;
		open_params->extra_header_data_size = imx_vpu_dec->codecdata_map_info.size;
	}


	/* open_params filled with valid data. Now we can actually open a new VPU
	 * decoder instance, as a contination from what we began in start().
	 * (The instance is closed in gst_imx_vpu_dec_context_close_decoder() ). */
	GST_DEBUG_OBJECT(decoder, "(re)opening decoder");
	if ((dec_ret = imx_vpu_api_dec_open(
		&(imx_vpu_dec->decoder),
		open_params,
		gst_imx_get_dma_buffer_from_memory(imx_vpu_dec->stream_buffer)
	)) != IMX_VPU_API_DEC_RETURN_CODE_OK)
	{
		GST_ERROR_OBJECT(imx_vpu_dec, "could not open decoder: %s", imx_vpu_api_dec_return_code_string(dec_ret));
		ret = FALSE;
		goto finish;
	}

	/* Create new context for the decoder. */
	imx_vpu_dec->decoder_context = gst_imx_vpu_dec_context_new(imx_vpu_dec->decoder);
	gst_object_ref_sink(GST_OBJECT_CAST(imx_vpu_dec->decoder_context));
	g_assert(imx_vpu_dec->decoder_context != NULL);

	/* Ref the codec state, to be able to use it later as reference
	 * for the gst_video_decoder_set_output_state() function. */
	imx_vpu_dec->input_state = gst_video_codec_state_ref(state);


	GST_DEBUG_OBJECT(decoder, "setting format finished");


finish:
	if (allowed_srccaps != NULL)
		gst_caps_unref(allowed_srccaps);

	return ret;
}


static GstFlowReturn gst_imx_vpu_dec_handle_frame(GstVideoDecoder *decoder, GstVideoCodecFrame *cur_frame)
{
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC_CAST(decoder);
	GstFlowReturn flow_ret;

	if (G_UNLIKELY(imx_vpu_dec->decoder == NULL))
	{
		GST_ERROR_OBJECT(imx_vpu_dec, "decoder was not initialized; cannot continue");
		gst_video_codec_frame_unref(cur_frame);
		imx_vpu_dec->fatal_error_cannot_decode = TRUE;
		return GST_FLOW_ERROR;
	}

	if (G_UNLIKELY(imx_vpu_dec->fatal_error_cannot_decode))
	{
		GST_ERROR_OBJECT(imx_vpu_dec, "fatal error previously recorded; cannot decode");
		gst_video_codec_frame_unref(cur_frame);
		return GST_FLOW_ERROR;
	}

	flow_ret = GST_FLOW_OK;

	if (G_LIKELY(cur_frame != NULL))
	{
		GstMapInfo in_map_info;
		ImxVpuApiEncodedFrame encoded_frame;
		ImxVpuApiDecReturnCodes dec_ret;

		gst_buffer_map(cur_frame->input_buffer, &in_map_info, GST_MAP_READ);

		encoded_frame.data = in_map_info.data;
		encoded_frame.data_size = in_map_info.size;
		encoded_frame.pts = cur_frame->pts;
		encoded_frame.dts = cur_frame->dts;
		/* The system frame number is necessary to correctly associate encoded
		 * frames and decoded frames. This is required, because some formats
		 * have a delay (= output frames only show up after N complete input
		 * frames), and others like h.264 even reorder frames. */
		encoded_frame.context = (void *)((guintptr)(cur_frame->system_frame_number));

		GST_LOG_OBJECT(imx_vpu_dec, "pushing frame with %zu bytes at virtual address %p into VPU (system frame number: %" G_GUINT32_FORMAT ")", encoded_frame.data_size, (gpointer)(encoded_frame.data), cur_frame->system_frame_number);

		dec_ret = imx_vpu_api_dec_push_encoded_frame(imx_vpu_dec->decoder, &encoded_frame);

		gst_buffer_unmap(cur_frame->input_buffer, &in_map_info);

		if (dec_ret != IMX_VPU_API_DEC_RETURN_CODE_OK)
		{
			GST_ERROR_OBJECT(imx_vpu_dec, "could not push input frame data into decoder: %s", imx_vpu_api_dec_return_code_string(dec_ret));
			flow_ret = GST_FLOW_ERROR;
			gst_video_codec_frame_unref(cur_frame);
			goto finish;
		}

		/* The GstVideoCodecFrame passed to handle_frame() gets ref'd prior
		 * to that call. Since we don't pass it directly to finish_frame(),
		 * drop_frame(), or release_frame() here (because we aren't done with
		 * it yet), we have to unref it here. We'll pull the frame from the
		 * GstVideoDecoder queue based on its system frame number later,
		 * and then we finish it. */
		gst_video_codec_frame_unref(cur_frame);
		cur_frame = NULL;
	}

	flow_ret = gst_imx_vpu_dec_decode_queued_frames(imx_vpu_dec);

finish:
	return flow_ret;
}


static gboolean gst_imx_vpu_dec_flush(GstVideoDecoder *decoder)
{
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC_CAST(decoder);

	if (imx_vpu_dec->decoder == NULL)
		return TRUE;

	/* decoder_context == NULL may happen with single-frame
	 * decoding like with WebP data or JPEG pictures. */
	if (imx_vpu_dec->decoder_context != NULL)
	{
		GST_IMX_VPU_DEC_CONTEXT_LOCK(imx_vpu_dec->decoder_context);
		imx_vpu_api_dec_flush(imx_vpu_dec->decoder);
		GST_IMX_VPU_DEC_CONTEXT_UNLOCK(imx_vpu_dec->decoder_context);
	}
	else
		imx_vpu_api_dec_flush(imx_vpu_dec->decoder);

	return TRUE;
}


static gboolean gst_imx_vpu_dec_drain(GstVideoDecoder *decoder)
{
	GstFlowReturn flow_ret;
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC_CAST(decoder);

	if (imx_vpu_dec->decoder == NULL)
		return GST_FLOW_OK;

	GST_INFO_OBJECT(imx_vpu_dec, "draining decoder");

	/* Drain by attempting to decode until the decoder runs out of data to decode. */
	flow_ret = gst_imx_vpu_dec_decode_queued_frames(imx_vpu_dec);
	if (flow_ret == GST_FLOW_EOS)
		flow_ret = GST_FLOW_OK;

	GST_INFO_OBJECT(imx_vpu_dec, "decoder drained");

	return flow_ret;
}


static GstFlowReturn gst_imx_vpu_dec_finish(GstVideoDecoder *decoder)
{
	GstFlowReturn flow_ret;
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC_CAST(decoder);

	if (imx_vpu_dec->decoder == NULL)
		return GST_FLOW_OK;

	if (G_UNLIKELY(imx_vpu_dec->fatal_error_cannot_decode))
		return GST_FLOW_OK;

	/* decoder_context == NULL may happen with single-frame
	 * decoding like with WebP data or JPEG pictures. */
	if (imx_vpu_dec->decoder_context != NULL)
	{
		GST_IMX_VPU_DEC_CONTEXT_LOCK(imx_vpu_dec->decoder_context);
		imx_vpu_api_dec_enable_drain_mode(imx_vpu_dec->decoder);
		GST_IMX_VPU_DEC_CONTEXT_UNLOCK(imx_vpu_dec->decoder_context);
	}
	else
		imx_vpu_api_dec_enable_drain_mode(imx_vpu_dec->decoder);

	GST_INFO_OBJECT(imx_vpu_dec, "pushing out all remaining unfinished frames");

	/* Finishing also involves draining the decoder. So, do this just like
	 * in gst_imx_vpu_dec_drain(). */
	flow_ret = gst_imx_vpu_dec_decode_queued_frames(imx_vpu_dec);
	if (flow_ret == GST_FLOW_EOS)
		flow_ret = GST_FLOW_OK;

	return flow_ret;
}


static gboolean gst_imx_vpu_dec_decide_allocation(GstVideoDecoder *decoder, GstQuery *query)
{
	GstImxVpuDec *imx_vpu_dec = GST_IMX_VPU_DEC_CAST(decoder);
	GstBufferPool *buffer_pool = NULL;
	GstStructure *pool_config;
	guint buffer_size, pool_index;
	GstCaps *negotiated_caps;
	GstVideoInfo video_info;
	GstImxDmaBufferAllocator *imx_dma_buffer_allocator = NULL;

	g_assert(imx_vpu_dec->decoder != NULL);


	/* Create and set up a DMA buffer pool if one does not exist already. */
	if (imx_vpu_dec->dma_buffer_pool == NULL)
	{
		GST_DEBUG_OBJECT(imx_vpu_dec, "no DMA buffer pool exists yet; creating one, and trying to use any i.MX DMA buffer allocator present in the query");

		gst_video_info_init(&video_info);
		gst_query_parse_allocation(query, &negotiated_caps, NULL);
		if (negotiated_caps != NULL)
		{
			GST_DEBUG_OBJECT(imx_vpu_dec, "negotiated caps in allocation query: %" GST_PTR_FORMAT, (gpointer)negotiated_caps);
			if (G_UNLIKELY(!gst_video_info_from_caps(&video_info, negotiated_caps)))
			{
				GST_ERROR_OBJECT(imx_vpu_dec, "caps cannot be converted to a video info structure");
				return FALSE;
			}
		}

		GST_DEBUG_OBJECT(decoder, "number of allocation buffer pools in query: %d", gst_query_get_n_allocation_pools(query));

		buffer_size = MAX(video_info.size, imx_vpu_dec->current_stream_info.min_fb_pool_framebuffer_size);

		/* Iterate over all pools and look for one that has an allocator
		 * which implements the GstImxDmaBufferAllocator interface. */
		for (pool_index = 0; pool_index < gst_query_get_n_allocation_pools(query); ++pool_index)
		{
			GstAllocator *candidate_allocator;
			GstBufferPool *current_buffer_pool;
			gboolean is_imx_dma_buffer_allocator = FALSE;

			gst_query_parse_nth_allocation_pool(query, pool_index, &current_buffer_pool, NULL, NULL, NULL);

			pool_config = gst_buffer_pool_get_config(current_buffer_pool);
			if (gst_buffer_pool_config_get_allocator(pool_config, &candidate_allocator, NULL))
				is_imx_dma_buffer_allocator = GST_IS_IMX_DMA_BUFFER_ALLOCATOR(candidate_allocator);

			GST_DEBUG_OBJECT(imx_vpu_dec, "buffer pool %p (#%u in allocation query) has an i.MX DMA buffer allocator: %d", (gpointer)current_buffer_pool, pool_index, is_imx_dma_buffer_allocator);

			gst_structure_free(pool_config);
			gst_object_unref(GST_OBJECT(current_buffer_pool));

			if (is_imx_dma_buffer_allocator)
			{
				imx_dma_buffer_allocator = GST_IMX_DMA_BUFFER_ALLOCATOR(candidate_allocator);
				break;
			}
		}

		/* If no suitable allocator was found, use ours. */
		if (imx_dma_buffer_allocator == NULL)
		{
			GST_DEBUG_OBJECT(imx_vpu_dec, "no buffer pool in the allocation query has an i.MX DMA buffer allocator; using the default one");
			imx_dma_buffer_allocator = GST_IMX_DMA_BUFFER_ALLOCATOR(imx_vpu_dec->default_dma_buf_allocator);
		}

		/* Now create our DMA buffer pool. */
		imx_vpu_dec->dma_buffer_pool = gst_imx_vpu_dec_buffer_pool_new(&(imx_vpu_dec->current_stream_info), imx_vpu_dec->decoder_context);
		/* Clear floating flag. */
		gst_object_ref_sink(GST_OBJECT(imx_vpu_dec->dma_buffer_pool));
		buffer_pool = GST_BUFFER_POOL(imx_vpu_dec->dma_buffer_pool);

		/* And configure our newly created pool. */
		pool_config = gst_buffer_pool_get_config(buffer_pool);
		gst_buffer_pool_config_set_params(pool_config, negotiated_caps, buffer_size, 0, 0);
		gst_buffer_pool_config_set_allocator(pool_config, GST_ALLOCATOR(imx_dma_buffer_allocator), NULL);
		gst_buffer_pool_config_add_option(pool_config, GST_BUFFER_POOL_OPTION_VIDEO_META);
		gst_buffer_pool_config_add_option(pool_config, GST_BUFFER_POOL_OPTION_IMX_VPU_DEC_BUFFER_POOL);
		gst_buffer_pool_set_config(buffer_pool, pool_config);
	}
	else
	{
		GST_DEBUG_OBJECT(imx_vpu_dec, "DMA buffer pool exists already; continuing to use it");

		buffer_pool = GST_BUFFER_POOL(imx_vpu_dec->dma_buffer_pool);
		pool_config = gst_buffer_pool_get_config(buffer_pool);
		gst_buffer_pool_config_get_params(pool_config, NULL, &buffer_size, NULL, NULL);
		gst_structure_free(pool_config);
	}


	/* Now let the parent class do its processing. */
	if (!GST_VIDEO_DECODER_CLASS(gst_imx_vpu_dec_parent_class)->decide_allocation(decoder, query))
		return FALSE;


	/* Make sure the DMA buffer pool is picked by setting
	 * it as the first in the allocacation pool list. */
	if (gst_query_get_n_allocation_pools(query) == 0)
	{
		GST_DEBUG_OBJECT(imx_vpu_dec, "there are no allocation pools in the allocation query; adding DMA buffer pool to it");
		gst_query_add_allocation_pool(query, buffer_pool, buffer_size, 0, 0);
	}
	else
	{
		GST_DEBUG_OBJECT(imx_vpu_dec, "there are allocation pools in the allocation query; setting the DMA buffer pool as the first one in the query");
		gst_query_set_nth_allocation_pool(query, 0, buffer_pool, buffer_size, 0, 0);
	}


	return TRUE;
}


static GstFlowReturn gst_imx_vpu_dec_decode_queued_frames(GstImxVpuDec *imx_vpu_dec)
{
	GstVideoDecoder *decoder = GST_VIDEO_DECODER_CAST(imx_vpu_dec);
	GstFlowReturn flow_ret = GST_FLOW_OK;
	gboolean do_loop = TRUE;
	ImxVpuApiDecReturnCodes dec_ret;
	ImxVpuApiDecOutputCodes output_code;

	do_loop = TRUE;

	do
	{
		if (imx_vpu_dec->fatal_error_cannot_decode)
			break;

		GST_TRACE_OBJECT(imx_vpu_dec, "decoding");


		/* Main decoding block. Lock the decoder context mutex to prevent
		 * race conditions between this decoding block and something else
		 * that tries to access the context at the same time. */

		if (imx_vpu_dec->decoder_context != NULL)
			GST_IMX_VPU_DEC_CONTEXT_LOCK(imx_vpu_dec->decoder_context);

		/* If decoded frames are NOT stored in one of the framebuffers from
		 * the VPU's pool, then we have to supply a buffer manually. If
		 * there currently is none, create it and set it up. */
		if (!(imx_vpu_dec->dec_global_info->flags & IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_DECODED_FRAMES_ARE_FROM_BUFFER_POOL)
		  && (imx_vpu_dec->prepared_output_buffer == NULL)
		  && (imx_vpu_dec->dma_buffer_pool != NULL)
		)
		{
			ImxDmaBuffer *dma_buffer;
			GstBufferPoolAcquireParams params = {
				.format = GST_FORMAT_DEFAULT,
				.start = 0,
				.stop = 0,
				.flags = 0
			};

			/* Note how we do NOT acquire with the GST_IMX_VPU_DEC_BUFFER_POOL_ACQUIRE_FLAG_SELECTED
			 * flag set. This is because we want to acquire the output buffer
			 * in a regular fashion, that is, we do not want it to be a
			 * "reserved" buffer (see the GstImxVpuDecBufferPool documentation
			 * for more about this). */
			flow_ret = gst_buffer_pool_acquire_buffer(GST_BUFFER_POOL_CAST(imx_vpu_dec->dma_buffer_pool), &(imx_vpu_dec->prepared_output_buffer), &params);

			if (G_LIKELY(flow_ret == GST_FLOW_OK))
			{
				dma_buffer = gst_imx_get_dma_buffer_from_buffer(imx_vpu_dec->prepared_output_buffer);
				if (G_UNLIKELY(dma_buffer == NULL))
				{
					gst_buffer_unref(imx_vpu_dec->prepared_output_buffer);
					imx_vpu_dec->prepared_output_buffer = NULL;
					GST_ERROR_OBJECT(imx_vpu_dec, "got gstbuffer from reserve_buffer(), but it does not contain a DMA buffer");
					flow_ret = GST_FLOW_ERROR;
				}
				else
				{
					GST_LOG_OBJECT(imx_vpu_dec, "acquired gstbuffer %p with DMA buffer %p", (gpointer)(imx_vpu_dec->prepared_output_buffer), (gpointer)dma_buffer);
					/* Set this DMA buffer as the output buffer, and the GstBuffer
					 * that houses that DMA buffer as the fb context so we can
					 * retrieve it later when a frame got decoded. That way, we
					 * know what GstBuffer the frame was decoded into. */
					imx_vpu_api_dec_set_output_frame_dma_buffer(imx_vpu_dec->decoder, dma_buffer, imx_vpu_dec->prepared_output_buffer);
				}
			}

			if (G_UNLIKELY(flow_ret != GST_FLOW_OK))
			{
				GST_ERROR_OBJECT(imx_vpu_dec, "could not prepare output buffer: %s", gst_flow_get_name(flow_ret));
				imx_vpu_dec->fatal_error_cannot_decode = TRUE;
				goto finish;
			}
		}

		dec_ret = imx_vpu_api_dec_decode(imx_vpu_dec->decoder, &output_code);

		if (imx_vpu_dec->decoder_context != NULL)
			GST_IMX_VPU_DEC_CONTEXT_UNLOCK(imx_vpu_dec->decoder_context);


		/* Now we evaluate the outcome of our decoding attempt. */

		if (G_UNLIKELY(dec_ret != IMX_VPU_API_DEC_RETURN_CODE_OK))
		{
			GST_ERROR_OBJECT(imx_vpu_dec, "decoding frames failed: %s", imx_vpu_api_dec_return_code_string(dec_ret));
			flow_ret = GST_FLOW_ERROR;
			imx_vpu_dec->fatal_error_cannot_decode = TRUE;
			goto finish;
		}

		/* No error occurred, so we can now evaluate the output code. */
		switch (output_code)
		{
			case IMX_VPU_API_DEC_OUTPUT_CODE_EOS:
				GST_DEBUG_OBJECT(imx_vpu_dec, "VPU reports EOS; no more frames to decode");
				flow_ret = GST_FLOW_EOS;
				do_loop = FALSE;
				break;

			case IMX_VPU_API_DEC_OUTPUT_CODE_FRAME_SKIPPED:
			{
				guint32 system_frame_number;
				GstVideoCodecFrame *skipped_frame;
				ImxVpuApiDecSkippedFrameReasons reason;
				void *skipped_frame_context;

				imx_vpu_api_dec_get_skipped_frame_info(imx_vpu_dec->decoder, &reason, &skipped_frame_context, NULL, NULL);

				system_frame_number = (guint32)((guintptr)skipped_frame_context);
				skipped_frame = gst_video_decoder_get_frame(decoder, system_frame_number);

				if (G_UNLIKELY(skipped_frame == NULL))
				{
					GST_WARNING_OBJECT(imx_vpu_dec, "no gstframe exists with number #%" G_GUINT32_FORMAT " - cannot handle skipped frame", system_frame_number);
					goto finish;
				}

				GST_LOG_OBJECT(imx_vpu_dec, "gst frame with number #%" G_GUINT32_FORMAT " was skipped by the decoder, reason: %s (%d)", system_frame_number, imx_vpu_api_dec_skipped_frame_reason_string(reason), reason);

				skipped_frame->output_buffer = NULL;

				switch (reason)
				{
					case IMX_VPU_API_DEC_SKIPPED_FRAME_REASON_INTERNAL_FRAME:
						GST_VIDEO_CODEC_FRAME_SET_DECODE_ONLY(skipped_frame);
						flow_ret = gst_video_decoder_finish_frame(decoder, skipped_frame);
						break;

					case IMX_VPU_API_DEC_SKIPPED_FRAME_REASON_CORRUPTED_FRAME:
					default:
						flow_ret = gst_video_decoder_drop_frame(decoder, skipped_frame);
						break;
				}

				break;
			}

			case IMX_VPU_API_DEC_OUTPUT_CODE_NEW_STREAM_INFO_AVAILABLE:
			{
				GstVideoFormat video_format;
				ImxVpuApiDecStreamInfo const *new_stream_info;
				gboolean is_semi_planar;
				gboolean is_interlaced;
				gboolean is_10bit;


				/* Retrieve the new stream info. */
				new_stream_info = imx_vpu_api_dec_get_stream_info(imx_vpu_dec->decoder);
				if (G_UNLIKELY(new_stream_info == NULL))
				{
					GST_ERROR_OBJECT(imx_vpu_dec, "could not get stream info for adding framebuffers to VPU");
					flow_ret = GST_FLOW_ERROR;
					imx_vpu_dec->fatal_error_cannot_decode = TRUE;
					goto finish;
				}

				imx_vpu_dec->current_stream_info = *new_stream_info;


				/* Process the new stream info. */

				is_semi_planar = !!(new_stream_info->flags & IMX_VPU_API_DEC_STREAM_INFO_FLAG_SEMI_PLANAR_FRAMES);
				is_interlaced = !!(new_stream_info->flags & IMX_VPU_API_DEC_STREAM_INFO_FLAG_INTERLACED);
				is_10bit = !!(new_stream_info->flags & IMX_VPU_API_DEC_STREAM_INFO_FLAG_10BIT);

				GST_DEBUG_OBJECT(
					imx_vpu_dec,
					"framebuffer metrics:  aligned size: %zux%zu pixel  actual size: %zux%zu pixel  stride: %zu",
					new_stream_info->decoded_frame_framebuffer_metrics.aligned_frame_width,
					new_stream_info->decoded_frame_framebuffer_metrics.aligned_frame_height,
					new_stream_info->decoded_frame_framebuffer_metrics.actual_frame_width,
					new_stream_info->decoded_frame_framebuffer_metrics.actual_frame_height,
					new_stream_info->decoded_frame_framebuffer_metrics.y_stride
				);
				GST_DEBUG_OBJECT(
					imx_vpu_dec,
					"allocation info:  fb pool framebuffers: %zu bytes  output framebuffers: %zu",
					new_stream_info->min_fb_pool_framebuffer_size,
					new_stream_info->min_output_framebuffer_size
				);
				GST_DEBUG_OBJECT(
					imx_vpu_dec,
					"additional stream info:  frame rate: %u/%u  min num required framebuffers: %zu  color format: %s  semi planar: %d  interlaced: %d  10 bit depth: %d",
					new_stream_info->frame_rate_numerator, new_stream_info->frame_rate_denominator,
					new_stream_info->min_num_required_framebuffers,
					imx_vpu_api_color_format_string(new_stream_info->color_format),
					is_semi_planar,
					is_interlaced,
					is_10bit
				);

				video_format = GST_VIDEO_FORMAT_UNKNOWN;

				switch (new_stream_info->color_format)
				{
					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV420_8BIT:
						video_format = GST_VIDEO_FORMAT_I420;
						break;

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV420_10BIT:
						video_format = GST_VIDEO_FORMAT_I420_10LE;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV420_8BIT:
						video_format = GST_VIDEO_FORMAT_NV12;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV420_10BIT:
#ifdef SUPPORTS_SEMI_PLANAR_10BIT_FRAMES
						video_format = GST_VIDEO_FORMAT_NV12_10LE40;
						break;
#else
						GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("GStreamer versions prior to 1.16 do not support the NV12_10LE40 format; 10-bit semi planar YUV 4:2:0 frames cannot be processed"), (NULL));
						flow_ret = GST_FLOW_ERROR;
						break;
#endif

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV411_8BIT:
						video_format = GST_VIDEO_FORMAT_Y41B;
						break;

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV411_10BIT:
						GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("GStreamer does not support 10bit YUV 4:1:1 data"), (NULL));
						flow_ret = GST_FLOW_ERROR;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV411_8BIT:
					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV411_10BIT:
						GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("GStreamer does not support semi planar YUV 4:1:1 data"), (NULL));
						flow_ret = GST_FLOW_ERROR;
						break;

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV422_HORIZONTAL_8BIT:
						video_format = GST_VIDEO_FORMAT_Y42B;
						break;

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV422_HORIZONTAL_10BIT:
						video_format = GST_VIDEO_FORMAT_I422_10LE;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV422_HORIZONTAL_8BIT:
						video_format = GST_VIDEO_FORMAT_NV16;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV422_HORIZONTAL_10BIT:
						/* GST_VIDEO_FORMAT_NV16_10LE32 is not the right format;
						 * we'd need something like GST_VIDEO_FORMAT_NV16_10LE40
						 * instead, which (currently) does not exist. */
						GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("GStreamer does not support semi planar 10bit YUV 4:2:2 horizontal data"), (NULL));
						flow_ret = GST_FLOW_ERROR;
						break;

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV422_VERTICAL_8BIT:
					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV422_VERTICAL_10BIT:
					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV422_VERTICAL_8BIT:
					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV422_VERTICAL_10BIT:
						GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("GStreamer does not support YUV 4:2:2 vertical data"), (NULL));
						flow_ret = GST_FLOW_ERROR;
						break;

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV444_8BIT:
						video_format = GST_VIDEO_FORMAT_Y444;
						break;

					case IMX_VPU_API_COLOR_FORMAT_FULLY_PLANAR_YUV444_10BIT:
						video_format = GST_VIDEO_FORMAT_Y444_10LE;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV444_8BIT:
						video_format = GST_VIDEO_FORMAT_NV24;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_YUV444_10BIT:
						/* There is no format like GST_VIDEO_FORMAT_NV24_10LE40. */
						GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("GStreamer does not support semi planar 10bit YUV 4:4:4 data"), (NULL));
						flow_ret = GST_FLOW_ERROR;
						break;

					case IMX_VPU_API_COLOR_FORMAT_SEMI_PLANAR_P010_10BIT:
						video_format = GST_VIDEO_FORMAT_P010_10LE;
						break;

					case IMX_VPU_API_COLOR_FORMAT_YUV400_8BIT:
						video_format = GST_VIDEO_FORMAT_GRAY8;
						break;

					case IMX_VPU_API_COLOR_FORMAT_YUV400_10BIT:
						/* GST_VIDEO_FORMAT_GRAY10_LE32 is probably not the right format;
						 * but since grayscale 10-bit data is unlikely to ever occur, it
						 * is not possible to be sure. */
						GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("GStreamer does not support 10-bit grayscale data"), (NULL));
						flow_ret = GST_FLOW_ERROR;
						break;

					default:
						break;
				}

				if (video_format == GST_VIDEO_FORMAT_UNKNOWN)
				{
					GST_ELEMENT_ERROR(imx_vpu_dec, STREAM, FORMAT, ("unsupported color format"), ("color format: %s (%d)", imx_vpu_api_color_format_string(new_stream_info->color_format), new_stream_info->color_format));
					flow_ret = GST_FLOW_ERROR;
				}

				if (flow_ret == GST_FLOW_ERROR)
				{
					imx_vpu_dec->fatal_error_cannot_decode = TRUE;
					goto finish;
				}


				/* Cleanup the old output state (if one exists) since it is
				 * likely to not be a match for our new stream info. Then set
				 * the new output state.
				 *
				 * Do this *before* calling gst_video_decoder_negotiate().
				 * This is because a valid output state is required
				 * for the negotiating process to work. */
				if (imx_vpu_dec->output_state != NULL)
				{
					gst_video_codec_state_unref(imx_vpu_dec->output_state);
					imx_vpu_dec->output_state = NULL;
				}

				/* Set the output state here, *before* negotiating.
				 * This is because a valid output state is required
				 * for the negotiating process to work. */
				imx_vpu_dec->output_state = gst_video_decoder_set_output_state(
					decoder,
					video_format,
					new_stream_info->decoded_frame_framebuffer_metrics.actual_frame_width,
					new_stream_info->decoded_frame_framebuffer_metrics.actual_frame_height,
					imx_vpu_dec->input_state
				);
				g_assert(imx_vpu_dec->output_state != NULL);


				/* Need to make sure the output state has the right interlac
				 * mode, which might not be set correctly in the input state. */

				GST_VIDEO_INFO_INTERLACE_MODE(&(imx_vpu_dec->output_state->info)) = is_interlaced ? GST_VIDEO_INTERLACE_MODE_MIXED : GST_VIDEO_INTERLACE_MODE_PROGRESSIVE;

				imx_vpu_dec->output_state->info.colorimetry.range = new_stream_info->video_full_range_flag ? GST_VIDEO_COLOR_RANGE_0_255 : GST_VIDEO_COLOR_RANGE_16_235;


				/* Fill colorimetry information into the output state. */

				if (new_stream_info->flags & IMX_VPU_API_DEC_STREAM_INFO_FLAG_COLOR_DESCRIPTION_AVAILABLE)
				{
					/* The integer values originate from ITU-T rec. H.265 Table E.3 */
					switch (new_stream_info->color_description.color_primaries)
					{
						case 1: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_BT709; break;
						case 4: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_BT470M; break;
						case 5: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_BT470BG; break;
						case 6: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_SMPTE170M; break;
						case 7: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_SMPTE240M; break;
						case 8: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_FILM; break;
						case 9: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_BT2020; break;
						default: imx_vpu_dec->output_state->info.colorimetry.primaries = GST_VIDEO_COLOR_PRIMARIES_UNKNOWN; break;
					}

					/* The integer values originate from ITU-T rec. H.265 Table E.4 */
					switch (new_stream_info->color_description.transfer_characteristics)
					{
						case 1: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_BT709; break;
						case 4: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_GAMMA22; break;
						case 5: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_GAMMA28; break;
						case 7: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_SMPTE240M; break;
						case 9: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_LOG100; break;
						case 10: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_LOG316; break;
						case 13: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_SRGB; break;
						default: imx_vpu_dec->output_state->info.colorimetry.transfer = GST_VIDEO_TRANSFER_UNKNOWN; break;
					}

					/* The integer values originate from ITU-T rec. H.265 Table E.5 */
					switch (new_stream_info->color_description.matrix_coefficients)
					{
						case 0: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_RGB; break;
						case 1: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_BT709; break;
						case 4: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_FCC; break;
						case 6: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_BT601; break;
						case 7: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_SMPTE240M; break;
						case 9: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_BT2020; break;
						case 10: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_BT2020; break;
						default: imx_vpu_dec->output_state->info.colorimetry.matrix = GST_VIDEO_COLOR_MATRIX_UNKNOWN; break;
					}
				}


				/* Clean up any output buffer and DMA buffer pool that may
				 * already exist, since these are likely to not be a match for
				 * our new stream info (maybe different width/height etc.). */

				if (imx_vpu_dec->prepared_output_buffer != NULL)
				{
					gst_buffer_unref(imx_vpu_dec->prepared_output_buffer);
					imx_vpu_dec->prepared_output_buffer = NULL;
				}

				/* Unref the dma_buffer_pool and set the pointer to NULL
				 * so that decide_allocation can create a new one. */
				if (imx_vpu_dec->dma_buffer_pool != NULL)
				{
					gst_object_unref(GST_OBJECT(imx_vpu_dec->dma_buffer_pool));
					imx_vpu_dec->dma_buffer_pool = NULL;
				}


				/* This is necessary to make sure decide_allocation
				 * is called, because this creates the dma_buffer_pool. */
				gst_video_decoder_negotiate(decoder);


				/* Output state and buffer pool are both set up
				 * at this point. Create and add the framebuffers now. */
				if (new_stream_info->min_num_required_framebuffers > 0)
				{
					if (!gst_imx_vpu_dec_allocate_and_add_framebuffers(imx_vpu_dec, new_stream_info->min_num_required_framebuffers))
					{
						flow_ret = GST_FLOW_ERROR;
						imx_vpu_dec->fatal_error_cannot_decode = TRUE;
						goto finish;
					}
				}

				break;
			}

			case IMX_VPU_API_DEC_OUTPUT_CODE_NEED_ADDITIONAL_FRAMEBUFFER:
			{
				GST_DEBUG_OBJECT(imx_vpu_dec, "additional framebuffer requested by decoder");

				if (!gst_imx_vpu_dec_allocate_and_add_framebuffers(imx_vpu_dec, 1))
				{
					flow_ret = GST_FLOW_ERROR;
					imx_vpu_dec->fatal_error_cannot_decode = TRUE;
					goto finish;
				}

				break;
			}

			case IMX_VPU_API_DEC_OUTPUT_CODE_DECODED_FRAME_AVAILABLE:
			{
				ImxVpuApiRawFrame decoded_frame;

				GST_IMX_VPU_DEC_CONTEXT_LOCK(imx_vpu_dec->decoder_context);

				dec_ret = imx_vpu_api_dec_get_decoded_frame(imx_vpu_dec->decoder, &decoded_frame);

				if (dec_ret == IMX_VPU_API_DEC_RETURN_CODE_OK)
				{
					guint32 system_frame_number;
					GstVideoCodecFrame *out_frame;
					GstVideoMeta *vmeta;
					GstBuffer *gstbuffer;


					/* Get the GstBuffer that holds the framebuffer the frame was decoded into.
					 * This can be either a framebuffer from the VPU's pool, or the output buffer
					 * we set earlier with imx_vpu_api_dec_set_output_frame_dma_buffer(), depending
					 * on the IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_DECODED_FRAMES_ARE_FROM_BUFFER_POOL
					 * flag being present or not. */
					gstbuffer = GST_BUFFER_CAST(decoded_frame.fb_context);
					g_assert(gstbuffer != NULL);

					/* And get the GstVideoCodecFrame that corresponds to the frame we just
					 * decoded. Since we identify each frame with a frame number, this works
					 * even with frame reordering. Because, when frames are reordered, what
					 * happens is that the numbers we get also got reordered (because they are
					 * the context of the frames we passed into the decoder). That way, we
					 * can fetch the correct GstVideoCodecFrame instances even with reodering. */
					system_frame_number = (guint32)((guintptr)(decoded_frame.context));
					out_frame = gst_video_decoder_get_frame(decoder, system_frame_number);

					if (G_UNLIKELY(out_frame == NULL))
					{
						GST_WARNING_OBJECT(imx_vpu_dec, "no gstframe exists with number #%" G_GUINT32_FORMAT " - discarding decoded frame", system_frame_number);

						imx_vpu_api_dec_return_framebuffer_to_decoder(imx_vpu_dec->decoder, decoded_frame.fb_dma_buffer);
						GST_IMX_VPU_DEC_CONTEXT_UNLOCK(imx_vpu_dec->decoder_context);

						break;
					}

					GST_LOG_OBJECT(imx_vpu_dec, "placing decoded frame into gst frame with number #%" G_GUINT32_FORMAT, system_frame_number);

					/* Set the GstVideoCodecFrame's output_buffer. Depending on the flag
					 * IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_DECODED_FRAMES_ARE_FROM_BUFFER_POOL
					 * being present or not, this is a GstBuffer that holds one of the VPU
					 * pool framebuffers, or a GstBuffer that holds the previously set
					 * output buffer (see above). In the former case, that GstBuffer will be
					 * a "reserved" buffer (see GstImxVpuDecBufferPool for more about this). */
					if (imx_vpu_dec->dec_global_info->flags & IMX_VPU_API_DEC_GLOBAL_INFO_FLAG_DECODED_FRAMES_ARE_FROM_BUFFER_POOL)
					{
						GstBufferPoolAcquireParams params = {
							.format = GST_FORMAT_DEFAULT,
							.start = 0,
							.stop = 0,
							.flags = (GstBufferPoolAcquireFlags)GST_IMX_VPU_DEC_BUFFER_POOL_ACQUIRE_FLAG_SELECTED
						};

						/* Select the reserved GstBuffer to make sure that the call to
						 * gst_buffer_pool_acquire_buffer() below acquires the correct one.
						 * We have to acquire the buffer even though we already have it,
						 * otherwise the GstImxVpuDecBufferPool release() function won't
						 * be called once all references to the GstBuffer are gone. This
						 * release() function _must_ be called because inside, it returns
						 * the framebuffer to the VPU's pool (which is essential, otherwise
						 * decoding will eventually fail because the VPU will run out of
						 * available framebuffers from its pool). */
						gst_imx_vpu_dec_buffer_pool_select_reserved_buffer(imx_vpu_dec->dma_buffer_pool, gstbuffer);

						flow_ret = gst_buffer_pool_acquire_buffer(GST_BUFFER_POOL_CAST(imx_vpu_dec->dma_buffer_pool), &(out_frame->output_buffer), &params);
					}
					else
					{
						out_frame->output_buffer = imx_vpu_dec->prepared_output_buffer;
						imx_vpu_dec->prepared_output_buffer = NULL;
					}

					GST_IMX_VPU_DEC_CONTEXT_UNLOCK(imx_vpu_dec->decoder_context);


					if (G_UNLIKELY(flow_ret != GST_FLOW_OK))
					{
						GST_ERROR_OBJECT(imx_vpu_dec, "could not allocate output frame: %s", gst_flow_get_name(flow_ret));
						gst_video_codec_frame_unref(out_frame);
						imx_vpu_dec->fatal_error_cannot_decode = TRUE;
						goto finish;
					}


					/* Set the interlacing flags in the videometa if necessary. */
					if (G_LIKELY((out_frame->output_buffer != NULL) && ((vmeta = gst_buffer_get_video_meta(out_frame->output_buffer)) != NULL)))
					{
						if (imx_vpu_dec->current_stream_info.flags & IMX_VPU_API_DEC_STREAM_INFO_FLAG_INTERLACED)
						{
							GST_BUFFER_FLAG_SET(out_frame->output_buffer, GST_VIDEO_BUFFER_FLAG_INTERLACED);
							vmeta->flags |= GST_VIDEO_FRAME_FLAG_INTERLACED;
						}
						else
						{
							GST_BUFFER_FLAG_UNSET(out_frame->output_buffer, GST_VIDEO_BUFFER_FLAG_INTERLACED);
							vmeta->flags &= ~GST_VIDEO_FRAME_FLAG_INTERLACED;
						}

						switch (decoded_frame.interlacing_mode)
						{
							case IMX_VPU_API_INTERLACING_MODE_TOP_FIELD_FIRST:
								GST_BUFFER_FLAG_SET(out_frame->output_buffer, GST_VIDEO_BUFFER_FLAG_TFF);
								GST_BUFFER_FLAG_UNSET(out_frame->output_buffer, GST_VIDEO_BUFFER_FLAG_ONEFIELD);
								vmeta->flags |= GST_VIDEO_FRAME_FLAG_TFF;
								vmeta->flags &= ~GST_VIDEO_FRAME_FLAG_ONEFIELD;
								break;

							case IMX_VPU_API_INTERLACING_MODE_TOP_FIELD_ONLY:
								GST_BUFFER_FLAG_SET(out_frame->output_buffer, GST_VIDEO_BUFFER_FLAG_TFF);
								GST_BUFFER_FLAG_SET(out_frame->output_buffer, GST_VIDEO_BUFFER_FLAG_ONEFIELD);
								vmeta->flags |= GST_VIDEO_FRAME_FLAG_TFF;
								vmeta->flags |= GST_VIDEO_FRAME_FLAG_ONEFIELD;
								break;

							default:
								break;
						}
					}


					/* We have finished processing the decoded frame. */
					flow_ret = gst_video_decoder_finish_frame(decoder, out_frame);
				}
				else
				{
					GST_IMX_VPU_DEC_CONTEXT_UNLOCK(imx_vpu_dec->decoder_context);
					GST_ERROR_OBJECT(imx_vpu_dec, "could not retrieve decoded frame: %s", imx_vpu_api_dec_return_code_string(dec_ret));
					goto finish;
				}

				break;
			}

			case IMX_VPU_API_DEC_OUTPUT_CODE_MORE_INPUT_DATA_NEEDED:
				GST_LOG_OBJECT(imx_vpu_dec, "VPU has no more data to decode");
				do_loop = FALSE;
				break;

			default:
				break;
		}
	}
	while (do_loop);

finish:
	return flow_ret;
}


static void gst_imx_vpu_dec_unref_decoder_context(GstImxVpuDec *imx_vpu_dec)
{
	if (imx_vpu_dec->decoder_context == NULL)
		return;

	/* Close the decoder right now to make sure it is closed by the
	 * time this call ends, even if there are more references to the
	 * decoder context somewhere. Otherwise, the imxvpuapi decoder
	 * would be closed only once all of these refs are unref'd and
	 * the decoder context finalizer kicks in. */
	gst_imx_vpu_dec_context_close_decoder(imx_vpu_dec->decoder_context);

	gst_object_unref(GST_OBJECT(imx_vpu_dec->decoder_context));
	imx_vpu_dec->decoder_context = NULL;
}


static gboolean gst_imx_vpu_dec_allocate_and_add_framebuffers(GstImxVpuDec *imx_vpu_dec, size_t num_framebuffers)
{
	size_t i;
	gboolean ret = TRUE;
	ImxVpuApiDecReturnCodes dec_ret;
	ImxDmaBuffer **dma_buffers = NULL;
	void **fb_contexts = NULL;

	g_assert(imx_vpu_dec->dma_buffer_pool != NULL);
	g_assert(num_framebuffers > 0);

	dma_buffers = g_malloc0(sizeof(ImxDmaBuffer *) * num_framebuffers);
	fb_contexts = g_malloc0(sizeof(void *) * num_framebuffers);

	GST_IMX_VPU_DEC_CONTEXT_LOCK(imx_vpu_dec->decoder_context);

	for (i = 0; i < num_framebuffers; ++i)
	{
		GstBuffer *reserved_buffer;
		ImxDmaBuffer *dma_buffer;

		reserved_buffer = gst_imx_vpu_dec_buffer_pool_reserve_buffer(imx_vpu_dec->dma_buffer_pool);
		if (G_UNLIKELY(reserved_buffer == NULL))
		{
			GST_ERROR_OBJECT(imx_vpu_dec, "could not reserve dma_buffer");
			ret = FALSE;
			goto finish;
		}

		dma_buffer = gst_imx_get_dma_buffer_from_buffer(reserved_buffer);
		if (G_UNLIKELY(dma_buffer == NULL))
		{
			gst_buffer_unref(reserved_buffer);
			GST_ERROR_OBJECT(imx_vpu_dec, "got gstbuffer from reserve_buffer(), but it does not contain a DMA buffer");
			ret = FALSE;
			goto finish;
		}
		dma_buffers[i] = dma_buffer;
		fb_contexts[i] = reserved_buffer;
	}

	if ((dec_ret = imx_vpu_api_dec_add_framebuffers_to_pool(imx_vpu_dec->decoder, dma_buffers, fb_contexts, num_framebuffers)) != IMX_VPU_API_DEC_RETURN_CODE_OK)
	{
		GST_ERROR_OBJECT(imx_vpu_dec, "could not add framebuffers to decoder pool: %s", imx_vpu_api_dec_return_code_string(dec_ret));
		ret = FALSE;
		goto finish;
	}


finish:
	/* The buffers that were allocated and reserved earlier by calling the
	 * gst_imx_vpu_dec_buffer_pool_reserve_buffer() function are NOT
	 * unref'd here. That is because they were allocated but not
	 * acquired. If the latter step is not done, they are not pooled
	 * properly, and therefore are not released into the buffer pool
	 * when they are unref'd.
	 *
	 * It turns out that it is not necessary to deallocate/unref these
	 * reserved buffers. That is because an error while adding framebuffers
	 * is fatal, will cause FALSE to be returned, and the callers will
	 * stop any attempts at decoding. So, the pipeline will be shut down,
	 * the buffer pool will be shut down as well, which in turn will cause
	 * it to internally unref any reserved buffers. In short, no resource
	 * leaks will occur. */

	GST_IMX_VPU_DEC_CONTEXT_UNLOCK(imx_vpu_dec->decoder_context);
	g_free(dma_buffers);
	g_free(fb_contexts);
	return ret;

	goto finish;
}




/* class_init function for autogenerated subclasses. */
static void derived_class_init(void *klass)
{
	GstImxVpuDecClass *imx_vpu_dec_class;
	GstElementClass *element_class;
	GstPadTemplate *sink_template;
	GstPadTemplate *src_template;
	GstCaps *sink_template_caps;
	GstCaps *src_template_caps;
	gboolean got_caps;
	gchar *longname;
	gchar *classification;
	gchar *description;
	gchar *author;
	ImxVpuApiCompressionFormat compression_format;
	GstImxVpuCodecDetails const *codec_details;

	imx_vpu_dec_class = GST_IMX_VPU_DEC_CLASS(klass);
	element_class = GST_ELEMENT_CLASS(klass);

	compression_format = (ImxVpuApiCompressionFormat)g_type_get_qdata(G_OBJECT_CLASS_TYPE(klass), gst_imx_vpu_compression_format_quark());
	codec_details = gst_imx_vpu_get_codec_details(compression_format);
	g_assert(codec_details != NULL);

	got_caps = gst_imx_vpu_get_caps_for_format(codec_details->compression_format, imx_vpu_api_dec_get_compression_format_support_details(codec_details->compression_format), &sink_template_caps, &src_template_caps, FALSE);
	g_assert(got_caps);

	sink_template = gst_pad_template_new("sink", GST_PAD_SINK, GST_PAD_ALWAYS, sink_template_caps);
	src_template = gst_pad_template_new("src", GST_PAD_SRC, GST_PAD_ALWAYS, src_template_caps);

	gst_element_class_add_pad_template(element_class, sink_template);
	gst_element_class_add_pad_template(element_class, src_template);

	imx_vpu_dec_class->requires_codec_data = codec_details->requires_codec_data;

	longname = g_strdup_printf("i.MX VPU %s video decoder", codec_details->desc_name);
	classification = g_strdup("Codec/Decoder/Video");
	description = g_strdup_printf("Hardware-accelerated %s video decoding using the i.MX VPU codec", codec_details->desc_name);
	author = g_strdup("Carlos Rafael Giani <crg7475@mailbox.org>");
	gst_element_class_set_metadata(element_class, longname, classification, description, author);
	g_free(longname);
	g_free(classification);
	g_free(description);
	g_free(author);
}


GTypeInfo gst_imx_vpu_dec_get_derived_type_info(void)
{
	GTypeInfo type_info =
	{
		sizeof(GstImxVpuDecClass),
		NULL,
		NULL,
		(GClassInitFunc)(void (*)(void))derived_class_init,
		NULL,
		NULL,
		sizeof(GstImxVpuDec),
		0,
		NULL,
		NULL
	};

	return type_info;
}


gboolean gst_imx_vpu_dec_register_decoder_type(GstPlugin *plugin, ImxVpuApiCompressionFormat compression_format)
{
	GType type;
	gchar *element_name, *type_name;
	gboolean ret = FALSE;
	GTypeInfo typeinfo = gst_imx_vpu_dec_get_derived_type_info();
	GstImxVpuCodecDetails const * codec_details = gst_imx_vpu_get_codec_details(compression_format);

	element_name = g_strdup_printf("imxvpudec_%s", codec_details->element_name_suffix);
	type_name = g_strdup_printf("GstImxVpuDec%s", codec_details->class_name_suffix);
	type = g_type_from_name(type_name);
	if (!type)
	{
		type = g_type_register_static(GST_TYPE_IMX_VPU_DEC, type_name, &typeinfo, 0);
		/* Set the compression format as qdata. The cast to a gpointer is safe,
		 * since we never dereference this "pointer" anyway. */
		g_type_set_qdata(type, gst_imx_vpu_compression_format_quark(), (gpointer)compression_format);
	}

	ret = gst_element_register(plugin, element_name, codec_details->rank, type);

	g_free(type_name);

	return ret;
}
